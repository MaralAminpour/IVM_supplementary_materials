{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOq6o53fBAcHpphAvVaz3y9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/IVM_supplementary_materials/blob/main/Clustering_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Application: Breast Cancer Diagnosis**\n",
        "\n",
        "\n",
        "Introducing the Wisconsin breast cancer dataset, a valuable tool in the field of medical research and diagnostics.\n",
        "\n",
        "Here's a glimpse of what's inside: Cells, both from benign and malignant tissues, were carefully extracted using a biopsy technique known as a **\"fine needle aspirate.\"** Once extracted, they were then captured and converted into **digital photographs** with the help of a **microscope**.\n",
        "\n",
        "But it doesn‚Äôt stop there! We used an intresting method named **\"snakes\"** to interactively **detect the boundaries of these cells.** With these boundaries set, we were able to extract a variety of cell features. We've got information on **30 different features**, such as:\n",
        "\n",
        "- **Radius**: This refers to the **average distance from the cell's center to its perimeter**.\n",
        "\n",
        "- **Texture**: It's essentially the **standard deviation of the cell's intensities**.\n",
        "\n",
        "- **Area**: It‚Äôs **the number of pixels enclosed **within the cell boundary.\n",
        "\n",
        "For each biopsy specimen, we've accumulated statistics related to these features. This includes the **average value**, **standard error**, and even the **highest value observed**.\n",
        "\n",
        "Here‚Äôs the crucial bit: Every specimen comes with a diagnosis, indicating whether it's benign (non-cancerous) or malignant (cancerous).\n",
        "\n",
        "Now, while our dataset focuses on the traditional method of **hand-crafted feature extraction from images**, it's worth noting that modern algorithms are evolving. Many now autonomously learn relevant features from images through tools like **convolutional neural networks**.\n",
        "\n",
        "But for this exploration, our primary focus will be on **uncovering the hidden structures within the feature space**. Let's dive in!"
      ],
      "metadata": {
        "id": "5w_sFf07jyyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised learning\n",
        "\n",
        "Unsupervised learning is a type of machine learning where the algorithm is provided with input data that doesn't have labeled responses. The system tries to learn the patterns and the structure from the data without any supervised feedback.\n",
        "\n",
        "In simpler terms, imagine you have a jigsaw puzzle without the finished picture on the box. You have to figure out how to piece it together based solely on the shapes and images on each piece. That's a bit like unsupervised learning.\n",
        "\n",
        "Some common types of unsupervised learning methods include:\n",
        "\n",
        "1. **Clustering:** It's like grouping. The aim is to segregate groups with similar traits and assign them into clusters. Examples include K-means clustering where we try to group data into 'K' number of clusters.\n",
        "\n",
        "2. **Association:** Here, the algorithm identifies rules that describe large portions of the data, like people that buy X also tend to buy Y (think of the \"customers who bought this item also bought\" feature on shopping sites).\n",
        "\n",
        "Unsupervised learning contrasts with supervised learning, where the algorithm is trained on a dataset where the correct outputs are known and the algorithm makes predictions based on this prior knowledge."
      ],
      "metadata": {
        "id": "w4yizCk3nMX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering\n",
        "\n",
        "Imagine you have a big box of mixed-up marbles, and you want to group them based on their similarities. That's kind of what clustering does in the world of machine learning.\n",
        "\n",
        "Now, clustering is a part of \"unsupervised learning\". It's like trying to sort these marbles **without someone telling you what criteria to use**. Maybe you'd choose color, size, or material. You're unsupervised and figuring it out on your own.\n",
        "\n",
        "In the specific method we're talking about, called **\"K-means clustering\"**, the goal is pretty straightforward. Imagine each group of marbles as having a **\"middle point\" or \"mean\"**. We want to group them such that the marbles are **as close as possible** to their respective **\"middle point\"**. The closer the marbles in a group are to this central point, the better our group is!\n",
        "\n",
        "There's a fancy way to measure how good our groups are using a thing called 'within-class variance'. Without diving too deep, it's a way to look at **how spread out marbles are in each group**. For our marble sorting, we want groups where marbles are really close to each other and the \"middle point\" of their group. So, **lower variance means our groups are tighter and better!**\n",
        "\n",
        "Here's a formula (don't worry too much about it) that captures this idea:\n",
        "\n",
        "$$\\sigma^2 = \\sum_{k=1}^{K} \\frac{n_k}{N} \\sigma_k^2$$\n",
        "\n",
        "This formula represents the **weighted average of the variances of each group** (or cluster) in the K-means clustering method.\n",
        "\n",
        "- $n$: number of examples in class k\n",
        "\n",
        "- $\\sigma$: variance if class k\n",
        "\n",
        "- $N$: total number of samples\n",
        "\n",
        "In simpler words, it's just a way to measure the average spread of marbles in all our groups. The goal is to get this value as small as possible!\n"
      ],
      "metadata": {
        "id": "O3vumVqClz7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will represent the intensity distribution for each cluster using a Gaussian distribution. This can be viewed as the likelihood of observing intensity $x_i$ from a sample in class k. This Gaussian distribution is characterized by two parameters: the mean $\\mu$ and the variance $\\sigma^2$.\n",
        "\n",
        "The observed intensity distribution of the entire image, represented by the normalized histogram, can be perceived as the likelihood of observing intensity x.\n",
        "\n",
        "We represent this likelihood distribution as a blend of Gaussian distributions, each multiplied by their respective mixing coefficients, and then summed across all classes.\n",
        "$$P(x_i|y_i=k,\\mu_k, \\sigma_k )=G_{\\sigma_k} (x_i‚àí\\mu_k)=\\frac{1}{\\sqrt{2\\pi \\sigma}} e^{\\frac{-(x_i-\\mu_k)^2}{2\\sigma_k^2}}$$\n",
        "$$P(x|\\Phi)=\\sum_{k} c_k G_{\\sigma_k} (x‚àí\\mu_k)$$\n",
        "The intensity distribution for each cluster k is Gaussian, described by $\\mu_k$ and $\\sigma_k$.\n",
        "\n",
        "The normalized intensity histogram can be depicted as a combination of Gaussians, defined by parameters $$\\Phi=(\\mu_k, \\sigma_k, c_k)_{k=1,\\ldots,K}.$$"
      ],
      "metadata": {
        "id": "9_jLQRWbhE5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Imagine you have a bunch of different colors, and you want to group them into certain categories based on how similar they are. In our case, we're grouping them based on their intensity, which is like how light or dark a color is.\n",
        "\n",
        "**1. Using Gaussian Distributions:**\n",
        "When we talk about representing the intensity distribution for each cluster (or category) using a Gaussian distribution, think of it like this: Imagine a bell curve. The top or peak of the curve represents the most common intensity for that group, and as we move away from the peak, the intensities become less common. This bell curve is defined by two things:\n",
        "- Its center (called the mean, represented by $\\mu$).\n",
        "- How spread out it is (called the variance, represented by $\\sigma^2$).\n",
        "\n",
        "**2. Likelihood of Observing an Intensity:**\n",
        "Now, when we see a certain intensity, $x_i$, in an image, we can use our bell curve to check how likely it is that this intensity belongs to a group $k$. The formula for this is:\n",
        "$$P(x_i|y_i=k,\\mu_k, \\sigma_k ) = \\frac{1}{\\sqrt{2\\pi \\sigma}} e^{\\frac{-(x_i-\\mu_k)^2}{2\\sigma_k^2}}$$\n",
        "This formula might look a bit intimidating, but it's just a mathematical way to describe our bell curve.\n",
        "\n",
        "**3. Mixture of Gaussians:**\n",
        "But what if our image has multiple groups of colors? This is where the idea of a mixture of Gaussians comes in. Instead of just one bell curve, we'll have several, each representing a different group of intensities. Each bell curve gets a weight, called the mixing coefficient ($c_k$), based on how dominant that group is in the image. We combine all these bell curves to get the overall distribution of intensities in the image:\n",
        "$$P(x|\\Phi) = \\sum_{k} c_k G_{\\sigma_k} (x‚àí\\mu_k)$$\n",
        "\n",
        "**4. Parameters for Each Gaussian:**\n",
        "Lastly, for every bell curve (or Gaussian) we use, it's described by its center ($\\mu_k$), spread ($\\sigma_k$), and weight ($c_k$). We collectively represent all these parameters for all the bell curves as\n",
        "$$\\Phi = (\\mu_k, \\sigma_k, c_k)_{k=1,\\ldots,K}$$.\n",
        "\n",
        "---\n",
        "\n",
        "So, in simple terms, we're using a combination of bell curves to represent the distribution of intensities in an image. Each bell curve stands for a group of similar intensities, and the entire set of bell curves helps us understand the overall color distribution in the image."
      ],
      "metadata": {
        "id": "r5RSdMarhKoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding K-means Clustering** (So how can we minimise the intra-class variance?)\n",
        "\n",
        "When we talk about K-means clustering, we're essentially trying to group data points into clusters where the data points in each cluster are similar to each other. One way to measure this similarity is by minimizing the differences (or variance) within each cluster.\n",
        "\n",
        "Here's a simple way to think about it: Imagine you're trying to organize books on a shelf. You want books of the same genre to be together. The \"distance\" between each book could be the difference in their topics. In K-means, we're doing something similar, but with data points instead of books.\n",
        "\n",
        "**The Process**\n",
        "\n",
        "1. **Initialization**: We start by randomly picking some points as the initial centers of our clusters.\n",
        "2. **Assigning Clusters**: For each data point, we check which of our cluster centers it's closest to in terms of its features (like the genre in our book example) and assign it to that cluster.\n",
        "3. **Recalculating Cluster Centers**: After assigning all points to clusters, we calculate the average of all points in a cluster and set this as the new center.\n",
        "4. **Repeat**: We go back to step 2 and continue until our cluster centers don't change much.\n",
        "\n",
        "A cool thing to note: The variance within each cluster is like the average \"distance\" of every data point in that cluster to its center. Our aim is to make this \"distance\" as small as possible.\n",
        "\n",
        "**The Math Behind It**\n",
        "\n",
        "The formula you provided calculates this \"distance\" or variance:\n",
        "\n",
        "$$ ùúé^2 = \\frac{1}{ùëÅ} ‚àë_(ùëò=1)^ùêæ ‚àë_(ùëñ‚ààùëÜ_ùëò) |ùíô_ùëñ‚àíùùÅ_ùëò |^2 $$\n",
        "\n",
        "Here:\n",
        "- \\(ùíô_ùëñ\\) represents the feature of a specific data point.\n",
        "- \\(ùùÅ_(ùëß_ùëñ)\\) is the average feature of the cluster to which the data point belongs.\n",
        "- The symbol |...| represents the Euclidean distance, which is just a way of measuring the straight-line distance between two points.\n",
        "\n",
        "In essence, this formula adds up all the distances of the data points to their respective cluster centers and then averages them.\n",
        "\n"
      ],
      "metadata": {
        "id": "Md6qTP0mnaJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More about the formula:\n",
        "\n",
        "$$ ùúé^2 = \\frac{1}{ùëÅ} ‚àë_(ùëò=1)^ùêæ ‚àë_(ùëñ‚ààùëÜ_ùëò) |ùíô_ùëñ‚àíùùÅ_ùëò |^2 $$\n",
        "\n",
        "1. **\\(ùúé^2\\)**: This represents the total within-class variance. Think of it as the average \"distance\" of each data point to the center of its assigned cluster.\n",
        "\n",
        "2. **‚àë_(ùëò=1)^ùêæ**: This is a summation symbol that runs from 1 to \\(ùêæ\\), where \\(ùêæ\\) is the number of clusters. It means we're adding up values for each cluster.\n",
        "\n",
        "3. **\\(ùëõ_ùëò\\)**: This represents the number of data points in cluster \\(ùëò\\).\n",
        "\n",
        "4. **ùëÅ**: This is the total number of data points across all clusters.\n",
        "\n",
        "5. **\\(ùëõ_ùëò/ùëÅ\\)**: This calculates the proportion of data points in cluster \\(ùëò\\) relative to the total number of data points.\n",
        "\n",
        "6. **1/ùëõ_ùëò**: This is the inverse of the number of data points in cluster \\(ùëò\\). It essentially gives weight to the variance based on cluster size.\n",
        "\n",
        "7. **‚àë_(ùëñ‚ààùëÜ_ùëò)**: Another summation, but this one sums over all the data points \\(ùëñ\\) that belong to cluster \\(ùëò\\).\n",
        "\n",
        "8. **|ùíô_ùëñ‚àíùùÅ_ùëò |^2**: This represents the squared Euclidean distance between a data point \\(ùíô_ùëñ\\) and the mean of its assigned cluster \\(ùùÅ_ùëò\\). It measures how far a data point is from the center of its cluster.\n",
        "\n",
        "9. **ùùÅ_(ùëß_ùëñ )**: This is the mean feature vector for the class assigned to data point \\(ùíô_ùëñ\\).\n",
        "\n",
        "Breaking it all down, the formula calculates the average of the squared distances of each data point to the center of its assigned cluster, and then sums these averages across all clusters to get the total within-class variance. It's a measure of how spread out the data points are within their assigned clusters."
      ],
      "metadata": {
        "id": "0wteaUBBrY-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **K-means Clustering and Its Limitations:**\n",
        "\n",
        "K-means clustering is a popular method for grouping data into distinct categories. It works by minimizing the distance of data points from the center of their assigned cluster. But it has a limitation: it always looks for round-shaped clusters. This means if our data naturally forms elongated or oddly-shaped groups, K-means might not capture them accurately.\n",
        "\n",
        "Imagine a group of points that forms a long, narrow ellipse. K-means would probably see this as multiple small round clusters rather than one long one.\n",
        "\n",
        "**Gaussian Mixture Model (GMM) - A Different Approach:**\n",
        "To tackle the above limitation, we can use the Gaussian Mixture Model (GMM). This model is excellent for cases where clusters aren't necessarily round. It can recognize and capture elongated or other complex shapes.\n",
        "\n",
        "In our example, when we look at two specific features - the mean area and the worst area - the Gaussian Mixture Model does a much better job than K-means. It recognizes the natural shape of the data and groups them more accurately. This leads to a high accuracy rate of 0.93.\n",
        "\n",
        "The secret behind GMM's performance is its ability to see clusters as shapes formed by Gaussian distributions. This means it can detect and adapt to clusters that are round, elongated, or even more intricate.\n",
        "\n",
        "\n",
        "So, if you're dealing with data that might not naturally form into neat circles, considering the Gaussian Mixture Model might be a good move!"
      ],
      "metadata": {
        "id": "kMHH2tImsO4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding the Gaussian Mixture Model (GMM) through MRI Brain Segmentation:**\n",
        "\n",
        "Gaussian Mixture Model (GMM) is a statistical model that represents the presence of multiple Gaussian distributions within a dataset. It assumes that the data is generated from several Gaussian distributions, each with its own parameters. Let's break it down using the example of segmenting brain MRI based on voxel intensities.\n",
        "\n",
        "1. **The Data: Brain MRI**\n",
        "   - An MRI scan of the brain contains voxel intensities. Each voxel (a 3D pixel) has an intensity value that represents a specific tissue type.\n",
        "   - If you were to plot the distribution of these intensities, you might notice distinct peaks or clusters.\n",
        "\n",
        "2. **Brainmasking and Observing Peaks:**\n",
        "   - Brainmasking is a preprocessing step where only the brain region is considered, and other parts are set to zero. This helps in removing non-brain elements.\n",
        "   - Once this is done, if you look at the histogram of intensities, you might notice three clear peaks. These peaks typically correspond to:\n",
        "     - White matter\n",
        "     - Grey matter\n",
        "     - Cerebro-spinal fluid (CSF)\n",
        "\n",
        "3. **How GMM Comes Into Play:**\n",
        "   - GMM will consider these peaks in the histogram as arising from different Gaussian distributions. It will try to fit multiple Gaussian curves to these peaks.\n",
        "   - The main goal of GMM is to identify parameters (like mean and standard deviation) for each Gaussian distribution (each tissue type in our case).\n",
        "   - Once GMM has these parameters, it can tell us the probability of a voxel belonging to white matter, grey matter, or CSF based on its intensity.\n",
        "\n",
        "4. **Processing the MRI Data:**\n",
        "   - For each voxel in the MRI data, GMM will evaluate its intensity and assign it to one of the Gaussian distributions (or tissue types) based on the highest probability.\n",
        "   - Remember the parts outside the brain that were padded with zeros after brainmasking? GMM will essentially ignore these because they don't belong to any of the identified Gaussian distributions.\n",
        "\n",
        "5. **Outcome:**\n",
        "   - After processing, GMM will provide segmented regions of the brain MRI, clearly demarcating areas of white matter, grey matter, and CSF.\n",
        "\n",
        "In summary, the Gaussian Mixture Model, through its ability to model multiple Gaussian distributions, can efficiently segment complex data like brain MRI images, identifying and categorizing different tissue types based on voxel intensities."
      ],
      "metadata": {
        "id": "7C_8quNPPlX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Modeling Intensity in Clusters:**\n",
        "   - Think of the intensity distribution for each cluster as a bell-shaped curve, called a Gaussian distribution.\n",
        "   - If we pick a random intensity value, say x_i, from a specific cluster (let's call it cluster k), the likelihood of that value coming from that cluster depends on two main things:\n",
        "\n",
        "     - The average or mean value of the intensities in that cluster (denoted as mu).\n",
        "\n",
        "     - The spread or variability of the intensities (denoted as sigma^2, which is the variance).\n",
        "\n",
        "2. **Understanding the Overall Image Intensity:**\n",
        "   - The overall intensity distribution of the image can be visualized as a histogram. This histogram, when scaled down so its total area equals 1, represents the probability of finding any given intensity, x, in the image.\n",
        "\n",
        "3. **Combining the Clusters' Distributions:**\n",
        "   - The combined probability distribution of the entire image is a mix of the individual Gaussian distributions from each cluster.\n",
        "   - This is where the term \"mixture of Gaussians\" comes from. For each intensity, x, we weigh each cluster's Gaussian distribution by a factor (called the mixing proportion) and add them all up.\n",
        "\n",
        "Here are the mathematical expressions for the above:\n",
        "   - The likelihood of observing intensity x_i for a sample from cluster k is given by:\n",
        "$$\n",
        "P(x_i | y_i=k, \\mu_k, \\sigma_k) = \\frac{1}{\\sqrt{2\\pi \\sigma}} e^{-(x_i-\\mu_k)^2 / 2\\sigma_k^2}\n",
        "$$\n",
        "   \n",
        "   - The combined probability distribution for the entire image's intensity is:\n",
        "$$\n",
        "P(x | \\Phi) = \\sum_k c_k \\frac{1}{\\sqrt{2\\pi \\sigma_k}} e^{-(x-\\mu_k)^2 / 2\\sigma_k^2}\n",
        "$$\n",
        "\n",
        "In simpler terms:\n",
        "- Each cluster's intensity distribution is like a bell curve with a specific average (mu_k) and spread (sigma_k).\n",
        "- The overall intensity distribution of the image is a mix of these bell curves, each weighted by a factor.\n",
        "\n",
        "Of course! Let's incorporate that.\n",
        "\n",
        "In the Gaussian Mixture Model, the mixing proportions (or weights) of each Gaussian distribution sum up to 1. This is crucial because it ensures that our model represents a proper probability distribution. Mathematically, this constraint is expressed as:\n",
        "$$\n",
        "\\sum_{k} c_k = 1\n",
        "$$\n",
        "Where:\n",
        "- \\( c_k \\) are the mixing proportions for each Gaussian distribution (or cluster) in the mixture.\n",
        "- The summation is taken over all clusters.\n",
        "\n",
        "In simpler terms, the combined weight of all individual bell curves (Gaussian distributions) in the mixture is equal to 1. This ensures that our model covers 100% of the intensity distribution.\n",
        "\n",
        "Lastly, when we talk about the normalized intensity histogram, it's essentially this mixture of Gaussian curves defined by the parameters (mu_k, sigma_k, c_k) for each cluster k.\n"
      ],
      "metadata": {
        "id": "_lp_dVToPyJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modeling Intensity in Clusters:**\n",
        "When you think about the intensity distribution for each cluster, imagine it as a bell-shaped curve, which we call a Gaussian distribution. Now, when you pick a random intensity value, say $x_i$, from a particular cluster (let's call this cluster k), the likelihood or probability of that value actually coming from cluster k depends on a couple of things. Firstly, the average or mean value of the intensities in that cluster, which we denote as $\\mu$. And secondly, the spread or how much the intensities vary in that cluster, which we represent as $\\sigma^2$ (this is known as the variance).\n",
        "\n",
        "**Understanding the Overall Image Intensity:**\n",
        "For the entire image, if you were to visualize its intensity distribution, it would look a bit like a histogram. When you scale this histogram so that its total area equals 1, what you're seeing is the probability of encountering a specific intensity, $x$, anywhere in the image.\n",
        "\n",
        "**Combining the Clusters' Distributions:**\n",
        "The combined or overall probability distribution of all the intensities in the image is actually a blend of these individual bell curves from each cluster. That's why we call it a \"mixture of Gaussians\". For every intensity $x$, we consider the Gaussian curve of each cluster, weight it by a certain factor (this is the mixing proportion), and then sum them all up.\n",
        "\n",
        "To give this a mathematical spin:\n",
        "The likelihood of observing an intensity $x_i$ from cluster k is expressed as:\n",
        "$$\n",
        "P(x_i | y_i=k, \\mu_k, \\sigma_k) = \\frac{1}{\\sqrt{2\\pi \\sigma}} e^{-(x_i-\\mu_k)^2 / 2\\sigma_k^2}\n",
        "$$\n",
        "\n",
        "And the combined probability distribution of the entire image, factoring in all these individual bell curves, can be described by:\n",
        "$$\n",
        "P(x | \\Phi) = \\sum_k c_k \\frac{1}{\\sqrt{2\\pi \\sigma_k}} e^{-(x-\\mu_k)^2 / 2\\sigma_k^2}\n",
        "$$\n",
        "\n",
        "In simpler words, each cluster's intensity distribution can be visualized as a bell curve, centered around a specific average (known as $\\mu_k$) with a certain spread ($\\sigma_k$). The image's overall intensity distribution is essentially a mixture of these bell curves, each given a certain importance or weight.\n",
        "\n",
        "By the way, in the Gaussian Mixture Model, there's a vital point to note: the weights of each of these Gaussian distributions add up to 1. This is captured by:\n",
        "$$\n",
        "\\sum_{k} c_k = 1\n",
        "$$\n",
        "\n",
        "This basically means that when we combine all the bell curves in the mixture, they represent the entire intensity distribution, or in other words, they cover 100% of it."
      ],
      "metadata": {
        "id": "cYxvt3Y9SRkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gaussian Mixture model\n",
        "\n",
        "The Gaussian Mixture Model, often shortened to GMM, is like a recipe with three main ingredients for each group or class. These ingredients are the mean (it's like the average value), the standard deviation (tells us how spread out the values are), and the mixing proportion (think of it as the importance or weight of each group).\n",
        "\n",
        "Imagine we have a brain image, and we're looking at the different intensities in it. What we want to do with the GMM is find the best mix ‚Äì the best set of means, standard deviations, and mixing proportions ‚Äì that describes these intensities. This is a bit like trying to find the perfect recipe that brings out all the flavors in a dish.\n",
        "\n",
        "To do this, we aim to maximize something called the log-likelihood of the image given our GMM parameters, denoted as $\\phi$. In simpler terms, we're trying to find the best fit for our model to the data. Now, here's a neat thing: if we assume that each tiny unit (or voxel) in the image is independent, we can sum up the individual contributions of each voxel to get the overall goodness of fit. And that's where our mix of Gaussian distributions comes into play.\n",
        "\n",
        "Our goal is to adjust our ingredients (the parameters of our GMM) so that we get the best possible fit. Mathematically, we can express this as:\n",
        "$$\n",
        "\\log L(\\Phi) = \\log P(\\mathbf{x}|\\Phi) = \\sum_{i=1}^{N} \\log P(x_i |\\Phi)\n",
        "$$\n",
        "Where $L(\\Phi)$ represents the likelihood of our image given our model parameters.\n",
        "\n",
        "To find the best fit, we can use some calculus magic! We'll set the derivatives (or slopes) of our log-likelihood with respect to each of our GMM parameters to zero. These equations are:\n",
        "$$\n",
        "\\frac{\\partial \\log L(\\Phi)}{\\partial \\mu_k} = 0\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial \\log L(\\Phi)}{\\partial \\sigma_k} = 0\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial \\log L(\\Phi)}{\\partial c_k} = 0\n",
        "$$\n",
        "\n",
        "Lastly, an essential part of our GMM is making sure the sum of all our mixing proportions equals 1, ensuring our model represents a complete probability distribution. This is captured as:\n",
        "$$\n",
        "\\sum_{k} c_k = 1\n",
        "$$\n",
        "\n",
        "That's how we fit a Gaussian Mixture Model to our image data. It's all about finding the right mix to best describe our image intensities."
      ],
      "metadata": {
        "id": "HXFDcRETTeoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analogy\n",
        "\n",
        "Imagine you have a colorful bag of candies. Each candy represents a data point (or intensity in our case), and they come in different flavors. Now, you want to group these candies based on their flavors, but you're not entirely sure how many flavors there are or what each flavor tastes like. That's where the Gaussian Mixture Model (GMM) steps in. It helps us find these hidden \"flavors\" (or clusters) in our data.\n",
        "\n",
        "To make things even cooler, GMM doesn't just say, \"Hey, this candy belongs to this flavor!\" Instead, it tells us the probability or chance that a candy belongs to each flavor. So, for a given candy, it might say there's an 80% chance it's apple-flavored and a 20% chance it's orange-flavored. This is what we call **probabilistic segmentation**.\n",
        "\n",
        "The entire game is about finding the best way to describe our candies using these groups or clusters. This is achieved using certain parameters, collectively called **–§ (Phi)**. Think of these parameters as the unique recipe for each flavor. The main ingredients of this recipe are:\n",
        "- The average taste of candies in a group (**mu**).\n",
        "- How varied the taste is within the group (**sigma**).\n",
        "- The overall presence of that flavor in the entire bag (**c**).\n",
        "\n",
        "Our mission is to find the best possible recipe for each flavor. But here's the thing: we don't just do it in one go. We keep tasting and adjusting until we're pretty sure we've got it right. This process of taste-test-adjust is what we call an **algorithm**. In this case, our algorithm is:\n",
        "1. Make a guess about the flavors and their recipes.\n",
        "2. Based on that guess, see how likely each candy is to belong to each flavor.\n",
        "3. Refine our guess based on the candies' responses.\n",
        "4. Repeat until our guess doesn't change much anymore (or in fancy terms, until the **log-likelihood** converges).\n",
        "\n",
        "Mathematically, our tastings and adjustments are expressed by these formulas:\n",
        "- For the chance that a candy belongs to a flavor:\n",
        "$$\n",
        "p_{ik} = \\frac{c_k G_{\\sigma_k} (x_i - \\mu_k)}{\\sum_k c_k G_{\\sigma_k} (x_i - \\mu_k)}\n",
        "$$\n",
        "- Refining our guess for the average taste (**mu**), taste spread (**sigma**), and flavor's presence (**c**):\n",
        "$$\n",
        "\\mu_k = \\frac{\\sum_i p_{ik} x_i}{\\sum_i p_{ik}}, \\quad \\sigma_k^2 = \\frac{\\sum_i p_{ik} (x_i - \\mu_k)^2}{\\sum_i p_{ik}}, \\quad c_k = \\frac{\\sum_i p_{ik}}{n}\n",
        "$$\n",
        "- Calculating the chance a candy belongs to a flavor using all our gathered information (via **Bayes formula**):\n",
        "$$\n",
        "P(y_i = k | x_i, \\Phi) = \\frac{p(y_i = k | \\Phi) P(x_i | y_i = k, \\Phi)}{\\sum_k p(y_i = k | \\Phi) P(x_i | y_i = k, \\Phi)}\n",
        "$$\n",
        "\n",
        "In essence, we're dancing between adjusting our guesses for the flavor recipes and checking how our candies feel about those guesses, over and over, until everyone's mostly happy!\n"
      ],
      "metadata": {
        "id": "2K-u7580Twk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "So, you've got this vibrant collection of data points, like having a bag of assorted candies. These data points, similar to candies, have their unique characteristics, like different flavors. Our challenge? Grouping these points based on their similarities. Enter Gaussian Mixture Model (GMM). It helps us identify these underlying groups in our data.\n",
        "\n",
        "But GMM adds a twist! Instead of just categorizing a data point into a group, it gives us the likelihood of it belonging to each group. For example, it might say a particular point has an 80% chance of being in Group A and 20% in Group B. This cool feature is known as **probabilistic segmentation**.\n",
        "\n",
        "The core idea is about best describing our data using these groups. We do this with certain parameters, known as $–§$ (Phi). These parameters help define each group. They include:\n",
        "- The average characteristic of points in a group ($mu$).\n",
        "- The variability within the group ($sigma$).\n",
        "- The proportion of that group in the entire dataset ($c$).\n",
        "\n",
        "Our goal is to pinpoint these parameters. But we don't do this randomly. It's a systematic process where we estimate, check, and refine. This iterative process is our **algorithm**. The steps are:\n",
        "1. Make an initial estimation of the groups and their characteristics.\n",
        "2. Based on that estimation, determine the likelihood of each data point belonging to each group.\n",
        "3. Tweak our initial guess based on the feedback.\n",
        "4. Keep refining until our estimates are consistent and reliable.\n",
        "\n",
        "On the math side of things, here's how we express this:\n",
        "- The likelihood a point belongs to a group:\n",
        "$$\n",
        "p_{ik} = \\frac{c_k G_{\\sigma_k} (x_i - \\mu_k)}{\\sum_k c_k G_{\\sigma_k} (x_i - \\mu_k)}\n",
        "$$\n",
        "- Refining our estimate for average ($mu$), variability ($sigma$), and group proportion ($c$):\n",
        "$$\n",
        "\\mu_k = \\frac{\\sum_i p_{ik} x_i}{\\sum_i p_{ik}}, \\quad \\sigma_k^2 = \\frac{\\sum_i p_{ik} (x_i - \\mu_k)^2}{\\sum_i p_{ik}}, \\quad c_k = \\frac{\\sum_i p_{ik}}{n}\n",
        "$$\n",
        "- Getting the likelihood of a point belonging to a group, considering all the parameters:\n",
        "$$\n",
        "P(y_i = k | x_i, \\Phi) = \\frac{p(y_i = k | \\Phi) P(x_i | y_i = k, \\Phi)}{\\sum_k p(y_i = k | \\Phi) P(x_i | y_i = k, \\Phi)}\n",
        "$$\n",
        "\n",
        "So there you have it! While it might feel like a dance between estimating and refining, by the end, it all comes together, giving us a clearer picture of our data's story. Dive into GMM and enjoy the journey!"
      ],
      "metadata": {
        "id": "x77s4Zz_Wk5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## what is feature extraction?\n",
        "\n",
        "\n",
        "Feature extraction is the art of simplifying our data. Imagine you have a vast canvas with a sprawling and complex painting. Feature extraction is like being able to reproduce that painting on a smaller canvas, while still capturing its essence and beauty. When we dive deep into large datasets, especially those from signals or images, there's so much going on! There are tons of details, but not all of them are crucial. Feature extraction helps us filter out the noise and focus on the salient features, those significant brush strokes that truly define the artwork.\n",
        "\n",
        "Why is this important? Well, when we reduce the complexity of our data, we're setting ourselves up for success in the world of machine learning. A simplified, yet meaningful dataset is less prone to overfitting, where our model might get too caught up in the tiny details and miss the big picture. Furthermore, models trained on such data are not only more accurate but also learn faster.\n",
        "\n",
        "So, the next time you come across a large dataset, think of it as that vast canvas. Through feature extraction, you're not only making the canvas more manageable but ensuring that every brush stroke on it truly counts. By emphasizing the right details and sidelining the redundant ones, we create a perfect setting for our machine learning models to thrive.\n"
      ],
      "metadata": {
        "id": "T0uzFrjEd2ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction Examples\n",
        "\n",
        "Picture this: You're watching a super detailed movie of your heart in action. This isn't just any regular film; it's captured using a special camera known as a 3D+T MRI. This camera allows us to see every tiny detail of your heart from all angles and across time.\n",
        "\n",
        "Now, these pictures can be a bit overwhelming. Every single detail of your heart is captured in these images. But, what if we just want to understand how your heart pumps blood? That's where \"feature extraction\" waltzes in like a detective, looking for clues about your heart's performance. It's like skimming a book to understand the main plot without getting bogged down by all the side stories.\n",
        "\n",
        "Now, to help us understand these images, we use a smart tool called a **convolutional neural network**. Think of it as a set of virtual glasses that can highlight specific parts of the heart in these pictures.\n",
        "\n",
        "From these highlighted sections, we measure how much blood the left ventricle (an important chamber in the heart) holds and pumps out. It's a bit like observing how much water fills and empties from a water balloon.\n",
        "\n",
        "Two moments capture our attention: when the balloon is fully blown (end diastole) and when it's most squeezed out (end systole). With these two measurements, we can calculate something called the \"ejection fraction\" (EF). It's like finding out how much water the balloon pushes out in one go!\n",
        "\n",
        "Here's a simple formula:\n",
        "$$\n",
        "EF = \\left( \\frac{LVEDV - LVESV}{LVEDV} \\right) \\times 100\n",
        "$$\n",
        "\n",
        "Here, we're just looking at the difference between the balloon's maximum and minimum water levels as a percentage. Don't worry about the letters too much; they're just fancy names for the balloon's maximum and minimum water levels.\n",
        "\n",
        "So, to wrap it up: We have this high-tech movie of the heart, we use special glasses to focus on the main events, and from that, we gauge how well our heart is working. Simple as that!\n"
      ],
      "metadata": {
        "id": "B_eUmfYEgsmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The formula for calculating the ejection fraction (EF) is given by:\n",
        "$$\n",
        "EF = \\left( \\frac{LVEDV - LVESV}{LVEDV} \\right) \\times 100\n",
        "$$\n",
        "\n",
        "Here's a breakdown of the terms:\n",
        "\n",
        "- **EF (Ejection Fraction)**: This represents the percentage of blood that's pumped out of the left ventricle (a main pumping chamber of the heart) during each heartbeat.\n",
        "\n",
        "- **LVEDV (Left Ventricular End-Diastolic Volume)**: This is the amount of blood in the left ventricle just before it contracts to pump the blood out. Essentially, it's the volume of the left ventricle when it's fullest.\n",
        "\n",
        "- **LVESV (Left Ventricular End-Systolic Volume)**: This is the amount of blood remaining in the left ventricle after it has contracted. It's the volume of the left ventricle when it's least full.\n",
        "\n",
        "The formula calculates the difference between the volume of blood in the left ventricle before and after it contracts. This difference is then divided by the full volume (before contraction) to get a fraction. Multiplying by 100 converts this fraction into a percentage, giving us the ejection fraction. This percentage helps in understanding how effectively the heart is pumping blood out during each beat."
      ],
      "metadata": {
        "id": "eMggHf8Vj-ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QRS Interval from ECG:\n",
        "\n",
        "To derive the QRS interval from an ECG signal, we start with processing the raw ECG. The first step involves filtering the signal to discard low-frequency drifts and any external noise. This gives us a cleaner waveform to work with. Next, we determine the peaks which appear as local highs and lows (maxima and minima) within the filtered signal. By setting certain thresholds, like a minimum height and distance between the peaks, we can accurately pinpoint the Q, R, and S peaks. After these peaks are detected, the next step is to compute the average duration of the QRS interval from the data gathered over the course of the ECG recording.\n",
        "\n",
        "+\n",
        "\n",
        "Let's chat about the QRS interval from an ECG, kind of like figuring out the rhythm of a catchy song! Imagine you're listening to a tune, but there's a bit of static. First, you'd want to get rid of that noise. That's what we do with filtering; it helps us hear (or in this case, see) the main beats clearly.\n",
        "\n",
        "Next up, we look for the high and low points in our 'song'. These are our peaks. But wait! Not every peak is the right beat we're looking for. So, we set some ground rules to pick out the main beats, which we call the Q, R, and S peaks.\n",
        "\n",
        "Lastly, with our main beats identified, we figure out the average duration between these beats. That's our QRS interval. It's like finding out the average time between the beats of our favorite song.\n",
        "\n",
        "And there you have it! From a jumbled tune to the clear rhythm of the heart's song!"
      ],
      "metadata": {
        "id": "-2Qmf1dVkZGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feature Categories:**\n",
        "\n",
        "Let‚Äôs now have a look at various types of features that we can use to train our machine learning models. Some features are directly observed or measured, and no feature extraction is needed. These are for example age, gender, height, or blood glucose levels.\n",
        "\n",
        "1. **Direct Observations:**\n",
        "   - Examples include age, gender, height, blood glucose levels, and others. These are straightforward measurements that don't need any additional processing.\n",
        "\n",
        "2. **Morphological Features:**\n",
        "   - These focus on the shape and structure of objects. Common ones are length, area, and volume. Think about measurements like the size of brain structures or the duration of QRS waves.\n",
        "\n",
        "3. **Texture Descriptions:**\n",
        "   - These dive into the statistical details of images or parts of images. Key terms here are mean, variance, kurtosis, and entropy. For instance, studying the texture of breast tissue samples can help detect cancer.\n",
        "\n",
        "4. **Transform-based Attributes:**\n",
        "   - This involves changing signals or images into different formats or domains. One example is moving to the frequency domain, which can help pinpoint abnormalities in things like ECG readings.\n",
        "\n",
        "5. **Local Feature Descriptions:**\n",
        "   - These emphasize specific points in images, like edges, blobs, or corners. They are typically highlighted using filtering techniques.\n",
        "\n",
        "In summary, there's a wide variety of features that assist in training machine learning models. Some are direct and easy to see, while others require a deeper look or transformation to understand fully."
      ],
      "metadata": {
        "id": "ULa0Efk6lS8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature extraction example\n",
        "\n",
        "\"We will be exploring the PatchCamelyon dataset, which includes histological images of breast tissues. These images showcase both healthy and cancerous cells. Our goal is to determine if texture descriptors, specifically GLCM statistical properties, or localized feature descriptors like DAISY can be indicators of cancer.\n",
        "\n",
        "**Breast Cancer Classification Using Histological Images**\n",
        "- **Dataset:** PatchCamelyon\n",
        "- **Feature Extraction Techniques:**\n",
        "  - Texture Descriptors (GLCM)\n",
        "  - Localised Feature Descriptors (DAISY)\n",
        "\n",
        "In the upcoming tutorial, you'll have the opportunity to delve into this yourself.\n",
        "\n",
        "**Jupyter Notebook:** 8.4 Feature Extraction\""
      ],
      "metadata": {
        "id": "idnxOvSwlkMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Texture Description\n",
        "\n",
        "The Gray level co-occurrence matrix, or GLCM for short, is a way to understand the texture of an image. Think of it like a table that shows how often pairs of pixels with specific brightness values are found together in an image.\n",
        "\n",
        "To make sense of the numbers in this table, we change them into percentages or probabilities. This makes it a 2D probability distribution, represented as \\( p(i,j) \\).\n",
        "\n",
        "Now, from this table, we can figure out some cool things about the texture of our image:\n",
        "\n",
        "1. **Contrast**: Measures the amount of local variations present. It's calculated using the formula:\n",
        "$$ \\sum_{i,j=0}^{l-1} (i-j)^2 p(i,j) $$\n",
        "\n",
        "2. **Dissimilarity**: Gives us an idea about how different pairs of pixels are. The formula is:\n",
        "$$ \\sum_{i,j=0}^{l-1} |i-j| p(i,j) $$\n",
        "\n",
        "3. **Homogeneity**: Tells us how close the elements of the matrix are to the matrix diagonal. In simpler words, how uniform our texture is. It's found using:\n",
        "$$ \\sum_{i,j=0}^{l-1} \\frac{1}{1+(i-j)^2} p(i,j) $$\n",
        "\n",
        "4. **Energy**: It's like a measure of uniformity or orderliness of the pixels. The higher the energy, the more order or regularity there is in the image. Formula:\n",
        "$$ \\sqrt{\\sum_{i,j=0}^{l-1} p(i,j)^2} $$\n",
        "\n",
        "5. **Correlation**: Shows how a pixel is related to its neighbor. The math behind it is a bit complex, but it's done using:\n",
        "$$ \\sum_{i,j=0}^{l-1} \\frac{(i-\\mu_i)(j-\\mu_j)}{\\sigma_I \\sigma_j} p(i,j) $$\n",
        "\n",
        "In simple words, once we've got our matrix set up and converted to percentages, these formulas help us describe the texture of our image in different ways!"
      ],
      "metadata": {
        "id": "Y1j0J5t9nYcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The DAISY descriptor\n",
        "\n",
        "Think of it as a special tool that doesn't just look for standout spots in an image but captures details all over it. It's super keen on understanding how colors change and transition in pictures by studying gradients from various angles.\n",
        "\n",
        "So, how does it work? First, we figure out how colors shift in different directions using some neat filtering tricks. After that, we add a gentle blur with varying degrees of fuzziness and pick certain spots to gather our information.\n",
        "\n",
        "Now, imagine a picture of the DAISY descriptor layout. There's a central red dot where the DAISY is located in the image. Then, surrounding it, you'll see red and blue dots. These are our special spots where we gather all the gradient details. And see those circles around? They symbolize the amount of blur we're using‚Äîthe bigger the circle, the fuzzier the blur. Plus, those lines coming out from the dots? They're showing us the gradient directions.\n",
        "\n",
        "When we're done, we line up all the cool features we've captured from DAISY into a neat list, ready for some action! We'll even see how DAISY helps in classifying medical samples for breast cancer and stack it up against another method called GLCM.\n",
        "\n",
        "A quick recap:\n",
        "DAISY:\n",
        "- Dots = where we're checking details\n",
        "- Circles = how much we're blurring things\n"
      ],
      "metadata": {
        "id": "FCHoVmY7oPx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature extraction in the era of deep learning\n",
        "\n",
        "**Feature Engineering:**\n",
        "This is like giving our computer a little manual on how to look at data. We're telling it, \"Hey, check out these particular bits and pieces, they're super important!\" In our chat today, I'll show you some cool techniques we've got up our sleeve for this.\n",
        "\n",
        "**Feature Learning:**\n",
        "Here's where things get futuristic! With deep neural networks, we basically let our computer play detective. Instead of us pointing out the clues, it finds them on its own. It's especially awesome for looking at things like pictures or sounds where there's a ton of info to sift through.\n",
        "\n",
        "**Why Still Love Feature Engineering?**\n",
        "Even with all the fancy auto-detective work, there's value in the old-school approach:\n",
        "\n",
        "- We get to be the boss! We decide what features the computer should focus on.\n",
        "\n",
        "- It's way easier to explain. If someone asks, \"Why did the computer decide this?\", we've got clear answers.\n",
        "\n",
        "- It's super handy when we don't have a mountain of data, or when we really want to understand what's driving our computer's decisions.\n",
        "\n",
        "In a nutshell, while letting the computer learn on its own is super powerful, there's still a special place in our hearts (and in many projects) for good old feature engineering!"
      ],
      "metadata": {
        "id": "X8RPtAy5ovE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, let's make this content a little more approachable with a friendly tone:\n",
        "\n",
        "---\n",
        "\n",
        "Hey there! üåô Ready to dive into the intriguing world of moon datasets? In today's session, we'll be unveiling the magic of spectral clustering. Have you ever wondered how we could cluster moon-shaped data? Would popular methods like K-means or Gaussian Mixture do the trick? Let's find out.\n",
        "\n",
        "So here's the thing: the moons dataset is already packed inside the scikit-learn library. Cool, right? Picture it as two curvy moon-shaped clusters just hanging out together. But the real question is: Can our good old pals K-means and Gaussian Mixture pinpoint these clusters? Well, it turns out that these clusters' shapes make it a tad tricky for K-means and Gaussian Mixture to work their magic, even though these moons don't overlap.\n",
        "\n",
        "Enter the hero of our story: spectral clustering! üåå Here's how it rolls:\n",
        "\n",
        "1. **Non-linear Manifold Embedding**: It starts with a cool move, performing a non-linear manifold embedding. Think of it as giving our data a fresh perspective. One popular method here is the Laplacian Eigenmap combined with... you guessed it, K-means clustering.\n",
        "\n",
        "2. **The Steps to Stardom**:\n",
        "    - **Step 1**: We begin by crafting a symmetric k-NN graph. In our case, it's set to the 5 nearest neighbors.\n",
        "    - **Step 2**: It's math time! We whip up the Graph Laplacian using the formula $L=D-A$.\n",
        "    - **Steps 3 & 4**: Eigenvalues and eigenvectors are our next stop. Order them from tiniest to mightiest.\n",
        "    - **Step 5**: The embedded space is where the action is! We take those eigenvectors and get moving.\n",
        "    - **Step 6**: Last but not least, K-means clustering steps in to identify our clusters.\n",
        "\n",
        "You with me? Cool. Let's remember how we crafted that non-linear manifold embedding.\n",
        "\n",
        "Fun Fact: The Graph Laplacian $L$ has a little secret. It carries a zero eigenvalue for each connected component. So, in our moon adventure, with two connected components, there are 2 zero eigenvalues, but we only invite one to the party.\n",
        "\n",
        "When we visualize our embedded dimensions, we can see that they do a stellar job of defining our clusters. So whether you're looking at the red moon or the blue moon, the dimensions have got you covered!\n",
        "\n",
        "But here's the exciting bit: spectral clustering is like a box of chocolates. There are many flavors (variants) to it:\n",
        "- You can craft the affinity matrix $A$ using various methods, from symmetric k-NN graphs to Gaussian kernels. Mix and match to find your favorite!\n",
        "- And the Graph Laplacian $L$? You can either go classic with $L=D-A$ or opt for a trendy normalized version.\n",
        "\n",
        "Guess what? Scikit-learn's SpectralClustering has got our backs. It comes with different types of affinity matrices. Whether you're in the mood for k-nearest neighbor-based affinity, a Gaussian kernel twist, or even your very own custom affinity, it's all there.\n",
        "\n"
      ],
      "metadata": {
        "id": "H5quacBCqNT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, my dear data explorer, are you ready to embark on this moonlit journey with spectral clustering? Let's make some magic happen!\n",
        "\n",
        "Hey there! üåå Let's journey into the intriguing world of spectral clustering. So, we've seen how it works with samples that have straightforward distances, like points on a graph. But what about stuff that isn‚Äôt as straightforward? Imagine trying to cluster whole images from baby brain MRIs! Sounds challenging, right?\n",
        "\n",
        "Picture this: We've got 68 adorable little ones, both term and preterm, all scanned when they reached term age. Now, if we want to get into the spectral clustering magic, we need to figure out this thing called the affinity matrix 'A'. Here's a fun twist: we line up all those brain images nice and neat, like ducks in a row, in the same reference space. Then, we gauge how similar they are using this cool method called normalised cross-correlation (NCC). And because all the images are of the same type, our NCC values are always on the positive side, maxing out at a perfect 1. It's like everyone holding hands in a fully connected circle of friends!\n",
        "\n",
        "We jazz things up a bit using the normalised Graph Laplacian since, you know, not all nodes are created equal. For our grand finale, we opt for a 3D view and, after some pondering over the data's layout, decide to group them into three fabulous clusters.\n",
        "\n",
        "So, that‚Äôs our exciting venture into the world of baby brain MRI clustering! Neat, right?"
      ],
      "metadata": {
        "id": "jNy7gIlEqRja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Imagine this: We've got a bunch of brain MRI images from both term and preterm babies, all scanned at a cozy 40 weeks GA. Now, to get a clearer view, think of averaging these images, kinda like blending different smoothie ingredients to get one delicious flavor.\n",
        "\n",
        "Here's the intriguing part:\n",
        "- **Clusters 1 & 2**: Dominated by term babies.\n",
        "- **Cluster 3**: Mainly preterm babies.\n",
        "\n",
        "Wondering why? Well, our preterm babies tend to have a bit more CSF (Cerebrospinal Fluid). And there's a twist! One of the term babies, who landed in Cluster 3, also had a bit more CSF, even though they weren't born preterm. It's like finding an unexpected ingredient in your smoothie!\n",
        "\n",
        "Now, if we splash some color on our clusters based on their birth gestational age, a pattern emerges. The third cluster lights up for preterm babies, while clusters 1 and 2? They're more of a mixed bag without a clear tie to the age at birth.\n",
        "\n",
        "To get a closer look, let's whip up an average of the images in each cluster. It's like viewing a movie's highlight reel! And the star of our show? The difference in CSF levels! Clusters 1 & 2 show less CSF, while cluster 3 has it in abundance, a common trait in our preterm babies.\n",
        "\n",
        "But remember our term baby outlier in cluster 3? On a closer look, this little champ has more CSF, making them more akin to the preterm babies, even though they arrived right on schedule."
      ],
      "metadata": {
        "id": "sZMOZ_FXq1HN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hey there! üåü Ever wonder why we select certain features over others? Well, there are some pretty cool reasons:\n",
        "\n",
        "1. **To Avoid Overfitting**: Think of it as decluttering. By removing extra stuff (or redundant data), our model doesn't get sidetracked by unnecessary noise.\n",
        "2. **Boost Accuracy**: It's like focusing on the main story without the side plots. By cutting out misleading data, our model can be more on-point and accurate.\n",
        "3. **Speed Things Up**: Everyone loves a quick result, right? With fewer features, our model can sprint through the training process, making everything snappier.\n"
      ],
      "metadata": {
        "id": "8M6HLTVvsMfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application: Prediction of age at scan\n",
        "\n",
        "Let's focus on predicting age based on brain scans. Remember when we talked about predicting age from those 86 brain structure volumes? Using just a straightforward multivariate linear regression might make our model a bit too eager and overfit the training data. The clue? A big difference in how the model does on the full training set versus when tested with cross-validation.\n",
        "\n",
        "Now, throughout our journey, we've come across several cool techniques to prevent this overfitting. Do any of them ring a bell? üõéÔ∏è\n",
        "\n",
        "- Keeping our model's enthusiasm in check with **regularisation** (like Ridge or Lasso).\n",
        "- Reducing the number of features with methods like **PCA, ICA, or Laplacian Eigenmap**.\n",
        "- Bringing together multiple models in **ensemble learning** (like our buddy, the Random Forest).\n",
        "- And of course, being picky with our data using **feature selection**!\n",
        "\n",
        "Quick recap: We aim to predict the age at scan using volumes of 86 brain structures in preterm babies. Just using multivariate linear regression? Oops, we overfit!\n",
        "\n",
        "So, what's in our toolkit to prevent overfitting? Regularisation, dimensionality reduction, ensemble learning, and feature selection. Got it? Onward!"
      ],
      "metadata": {
        "id": "salPD6QqsPMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## simulated features\n",
        "\n",
        " Imagine we've got five of these features, each sprinkled with varying amounts of noise and non-linearity. Picture it like this:\n",
        "\n",
        "- The x-axis is showing the feature value.\n",
        "- The y-axis? That‚Äôs our target value.\n",
        "\n",
        "Now, our goal is to pick the best and most informative features. To help visualize this, we've simulated five features:\n",
        "\n",
        "1. The first feature has a straight-line relationship with the targets, but it's kinda like static on a TV ‚Äì a bit noisy.\n",
        "2. The second one? Still a straight-line relationship, but clearer than a sunny day ‚Äì much less noise.\n",
        "3. Our third feature's relationship with the target is more like a wavy line, a bit curvy but still pretty clear.\n",
        "4. The fourth one is like a roller coaster, super curvy and unpredictable. It‚Äôs not a straight shot to our target, but it‚Äôs clear without much noise.\n",
        "5. And the fifth? Well, that's just like radio static ‚Äì all noise and no clear connection to our targets."
      ],
      "metadata": {
        "id": "KzJfcGXvtdh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature importances\n",
        "\n",
        "Feature importances helps us understand which features really make a difference when making predictions.\n",
        "\n",
        "**Univariate Feature Importances**:\n",
        "- Think of this as looking at one feature at a time.\n",
        "- We're trying to see how good each feature is at predicting the target values.\n",
        "- It‚Äôs kinda like finding out which ingredients make a dish taste amazing! We might use things like correlation or mutual information to get a feel.\n",
        "\n",
        "**Model-Based Feature Importances**:\n",
        "- Imagine building a LEGO castle. Each brick represents a feature, and some bricks are more crucial than others.\n",
        "- If we‚Äôre talking linear models (like regressors or classifiers), we create a fancy formula that looks like this:\n",
        "  $$y-hat = w_0 + w_1 x_1 + ... + w_n x_n$$\n",
        "  Here, the weights (those 'w' values) tell us the importance of each feature.\n",
        "- Quick tip: Make sure to scale your features before fitting; it‚Äôs like making sure all the LEGO bricks are the same size.\n",
        "- If you‚Äôre using scikit-learn, you can peek at these weights with `model.coef_`.\n",
        "\n",
        "**Tree-Based Methods**:\n",
        "- Trees are cool! They split data based on features and see which ones clear up confusion the best.\n",
        "- If you‚Äôve got a bunch of trees (like in a forest), you average out the results from all trees.\n",
        "- Again, if you're using scikit-learn, check out `model.feature_importances_` to see the star players.\n"
      ],
      "metadata": {
        "id": "wCQIts2cts6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Univariate feature selection\n",
        "\n",
        "1. **What's the gist?** We want to pick out the most relevant features for our data analysis. One way to do this is by looking at the Pearson‚Äôs correlation coefficient.\n",
        "2. **How do we use it?** Think of this coefficient as a measure of how related two sets of data are. If a feature has a coefficient close to +1 or -1, it means it's strongly correlated with our target outcome.\n",
        "3. **But wait, there's noise!** Sometimes, features might not show a clear linear relationship because of noise or non-linear patterns. For instance, a very wiggly feature or one with lots of random spikes might not have a strong correlation.\n",
        "4. **So how do we pick the best features?** With univariate feature selection, we rank the features based on their importance and pick the top ones. Tools like `SelectKBest` from `scikit-learn` can help with this.\n",
        "5. **A note on scikit-learn:** While it doesn't calculate the Pearson‚Äôs coefficient directly, it has a nifty function called `f_regression`. This gives us an F-value, which essentially tells us how likely it is that a feature's correlation with the target is just by chance. The good news? This F-value is closely tied to the Pearson‚Äôs coefficient.\n",
        "6. **Using SelectKBest:** This tool needs two things from us - a way to score features (like our `f_regression` function) and the number of top features we want.\n",
        "7. **What did we find?** For our data, the top three features (0, 1, and 2) have a straightforward, linear relationship with our target values."
      ],
      "metadata": {
        "id": "p3Ovd1f2uus5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Univariate feature selection2\n",
        "\n",
        "Ready to find out which features are the best buddies with your target?\n",
        "\n",
        "1. **What's Mutual Information?** Think of it like a friendship meter. It tells us how much two variables, in our case, feature values and target values, have in common. The more they \"know\" about each other, the higher the mutual information!\n",
        "2. **How to pick the best buddies?** We're on the lookout for features that share a lot of secrets (information) with our target. But sometimes, the chatter (noise) can get in the way and lessen their bond.\n",
        "3. **Unique and Wiggly Relations? No Problem!** The cool thing about mutual information is that it's unfazed by whether the relationship is straight or all over the place. In fact, a super wiggly feature can sometimes share more info with the target than a straight-line feature.\n",
        "4. **How to Measure Friendship in Code?** `Scikit-learn` is our matchmaker here! It has a tool called `mutual_info_regression` that measures how tight-knit our features and targets are. If we were to pick the top three BFFs using `SelectKBest`, it'd likely be features 1, 2, and 3!\n",
        "5. **But wait, what about Classification?** Ah, for that, `scikit-learn` has different measuring tapes. The top ones are:\n",
        "   - `chi2` (chi squared)\n",
        "   - `f_classif`\n",
        "   - `mutual_info_classif`\n"
      ],
      "metadata": {
        "id": "rmWHp1E6vAtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model based feature selection\n",
        "\n",
        "**What's the Lasso Way?**\n",
        "\n",
        "1. Lasso is like a talent scout. It zeroes in on the most impactful features by giving them the highest \"coefficients\" or weights.\n",
        "2. There's some magic behind the scenes! When Lasso uses the L1 norm penalty, it makes many feature weights zero, resulting in a simpler model. This is called \"sparsity.\"\n",
        "\n",
        "**Tuning with LassoCV:**\n",
        "\n",
        "Lasso has this cool feature called \"LassoCV.\" It's like a radio that automatically tunes to the best station! In our case, it finds the ideal setting for the hyperparameter ùúÜ. And for our little example here, it settled on 0.003.\n",
        "\n",
        "**Picking the Stars with SelectFromModel:**\n",
        "\n",
        "Sklearn has this nifty tool called `SelectFromModel`. Think of it as a director casting the leading roles in a movie. It selects features based on how impactful (or high) their coefficients are. And yep, for our movie, features 1 and 2 got the leading roles! They're pretty straightforward and don't come with a lot of drama (noise)."
      ],
      "metadata": {
        "id": "zRfqTjzGvtOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model based feature selection\n",
        "\n",
        "So, with Random Forest guiding us, we're equipped to uncover those truly special features that can make our predictions shine!\n",
        "\n",
        "Let's venture into the world of \"Random Forest\" for feature selection!\n",
        "\n",
        "**The Random Forest Way:**\n",
        "\n",
        "1. Random Forest, like a wise old sage, picks features based on their \"importance.\" How do they get this wisdom? It's by calculating how much each feature clears up the uncertainty (or decreases the impurity) in a decision tree.\n",
        "2. Picture a bunch of trees, each with their own opinions. Random Forest's \"feature importance\" is the average clarity (or decrease in impurity) each feature brings across all these trees.\n",
        "\n",
        "**Selecting the Shining Stars:**\n",
        "\n",
        "Using `SelectFromModel`, we can handpick those standout features. Just like before, but this time, it's based on `feature_importances_`. And for this act, features 1, 2, and 3 stole the show!\n",
        "\n",
        "**A Special Note on Feature 3:**\n",
        "\n",
        "Isn't it fascinating? Even though Feature 3 dances to its own non-linear tune and doesn't have a unique bond with the target, Random Forest still recognizes its worth! It's like appreciating a unique dancer in a troupe. Teaming up with Feature 1, they create a magical performance.\n"
      ],
      "metadata": {
        "id": "Qj9HblnqwaE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recursive Feature Elimination\n",
        "\n",
        "Peeling the onion layer by layer, that's what \"Recursive Feature Elimination\" (RFE) is all about!\n",
        "\n",
        "**Deep Dive into RFE:**\n",
        "\n",
        "1. RFE doesn't just pick features; it *ranks* them. How? By starting with all features and taking them away, one by one, from least important to most.\n",
        "2. Here's the fun part: This isn't a one-time deal. With each feature removed, the model retrains and recalculates the importances. Think of it as a reality show, where contestants are voted off one by one, based on their performance in each episode.\n",
        "\n",
        "**Steps in RFE's Dance:**\n",
        "\n",
        "1. First, we fit a ranking model. For our case, we're using Ridge regression.\n",
        "2. After that, we find the least important feature and bid it goodbye.\n",
        "3. Rinse and repeat! We keep training the model with the remaining features and keep eliminating the weakest links.\n",
        "\n",
        "By the end, we get a ranking of all features from the star performers (last ones standing) to the early departures (first ones out).\n",
        "\n",
        "**Special Tools for RFE:**\n",
        "\n",
        "If you're a fan of automation, you'd love `RFECV` from sklearn. It's RFE with a sprinkle of cross-validation magic. Instead of manually picking the number of features, `RFECV` finds the optimal number that maximizes model performance.\n",
        "\n",
        "For our Ridge regression escapade, features 1 and 2 came out on top. Bravo!"
      ],
      "metadata": {
        "id": "0LTv90rlxJ85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature selection results - summary\n",
        "\n",
        "Time for a feature selection showdown summary!\n",
        "\n",
        "**The Stars of the Show:**\n",
        "\n",
        "- Features 1 and 2 are the headliners, chosen by all the methods. They're as straight as an arrow, which makes them easy to work with, and they keep the noise level down.\n",
        "\n",
        "**Feature 3, the Underdog:**\n",
        "\n",
        "- Feature 3 may not have a unique relationship with the target, but it's a versatile player. Mutual information and Random Forest recognized its value. It brings a dose of non-linearity to the party and keeps the noise to a minimum.\n",
        "- In fact, it even outshone Feature 2 in the eyes of Random Forest and Mutual Information!\n",
        "\n",
        "**The Noise Maker, Feature 4:**\n",
        "\n",
        "- Last but not least, Feature 4 got the boot every time. It's like that one noisy neighbor that no one wants to invite to the party. Just full of noise, no real connection to the target.\n",
        "\n",
        "So, there you have it! Our feature selection methods have spoken, and the winners are Features 1, 2, and 3. They bring the harmony, while Feature 4 got left out in the noise."
      ],
      "metadata": {
        "id": "aZeq2_6xx3Sy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application: Prediction of age at scan\n",
        "\n",
        "Let's circle back to our brainy task of predicting age at scan for preterm neonates using volumes of 86 brain structures.\n",
        "\n",
        "**The Overfitting Conundrum:**\n",
        "\n",
        "You see, when we initially tried to tackle this with multivariate linear regression, it was like trying to fit a square peg into a round hole - overfitting was lurking around the corner.\n",
        "\n",
        "**Enter Feature Selection, Our Hero!**\n",
        "\n",
        "But hold on, can feature selection actually save the day and prevent overfitting?\n",
        "\n",
        "**Drumroll, Please... Conclusion Time!**\n",
        "\n",
        "The answer is a resounding YES! Feature selection turned out to be our trusty sidekick in this adventure. It helped us avoid overfitting and led to some impressive results.\n",
        "\n",
        "We tested three feature selection methods:\n",
        "\n",
        "1. Univariate feature selection using correlation (picking 2 features).\n",
        "2. Model-based feature selection using Lasso (choosing 6 features).\n",
        "3. Recursive Feature Elimination (RFE) with a linear regression model (opting for 4 features).\n",
        "\n",
        "And the winner of this feature selection showdown is... Lasso! It performed the best, closely followed by RFE. These methods worked their magic and gave us results akin to using Ridge Regression, a pretty effective model.\n",
        "\n",
        "So there you have it! Feature selection not only prevents overfitting but also boosts our performance. It's like finding the perfect-sized puzzle piece for our prediction task."
      ],
      "metadata": {
        "id": "cA-Y_5T-yS8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretation of feature importances\n",
        "\n",
        "**Understanding Feature Importances:**\n",
        "\n",
        "In the realm of machine learning, feature importances provide us with valuable insights into which features carry the most weight when it comes to making predictions. These insights can be enlightening and help us better understand the problem we're trying to solve.\n",
        "\n",
        "**The Challenge of Correlated Features:**\n",
        "\n",
        "However, there's a challenge we often face. Imagine you have a group of features that are closely related or correlated. It's like having multiple teammates who are equally skilled. Sometimes, when we use feature selection methods, they might end up choosing just one of these correlated features to represent the entire group. This can be a problem because some of those highly predictive features might not get the recognition they deserve, leading to lower importances for them.\n",
        "\n",
        "**Comparing Feature Selection Methods:**\n",
        "\n",
        "Let's take a look at an example where we used three different feature selection methods. Each of these methods selected its own set of top three features. This variety in selection can make it tricky to interpret which features are truly the most important.\n",
        "\n",
        "So, in a nutshell, feature importances are like guiding lights in the world of data, showing us which features matter most. But when features are closely connected, we might end up highlighting just one, and that can sometimes overshadow other valuable features. It's all part of the data exploration journey!"
      ],
      "metadata": {
        "id": "Ld01MWZiy4YZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature interpretation\n",
        "\n",
        "The instability we encounter here makes it tricky to confidently interpret the significance of individual features. However, if our primary aim is to reduce dimensionality rather than dissect individual features, this instability might not be a big concern.\n",
        "\n",
        "There are some clever solutions to address this issue:\n",
        "\n",
        "1. **Stability Selection:** This involves running the feature selection method on various subsets of the data and then calculating statistics to determine how often each feature is selected.\n",
        "\n",
        "2. **Feature Merging:** Another approach is to identify pairs or clusters of highly correlated features and make a decision. You can either drop the less predictive ones or merge them into a single representative feature.\n",
        "\n",
        "These strategies help us navigate the instability and make the most out of our feature selection process."
      ],
      "metadata": {
        "id": "aOrOVbJ4zlV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comic time\n",
        "\n"
      ],
      "metadata": {
        "id": "-1h2KpMtz4yU"
      }
    }
  ]
}