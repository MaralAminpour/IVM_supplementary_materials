{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMf8psVaIH2GTokffbtCZi3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/IVM_supplementary_materials/blob/main/Clustering_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will represent the intensity distribution for each cluster using a Gaussian distribution. This can be viewed as the likelihood of observing intensity $x_i$ from a sample in class k. This Gaussian distribution is characterized by two parameters: the mean $\\mu$ and the variance $\\sigma^2$.\n",
        "\n",
        "The observed intensity distribution of the entire image, represented by the normalized histogram, can be perceived as the likelihood of observing intensity x.\n",
        "\n",
        "We represent this likelihood distribution as a blend of Gaussian distributions, each multiplied by their respective mixing coefficients, and then summed across all classes.\n",
        "$$P(x_i|y_i=k,\\mu_k, \\sigma_k )=G_{\\sigma_k} (x_i−\\mu_k)=\\frac{1}{\\sqrt{2\\pi \\sigma}} e^{\\frac{-(x_i-\\mu_k)^2}{2\\sigma_k^2}}$$\n",
        "$$P(x|\\Phi)=\\sum_{k} c_k G_{\\sigma_k} (x−\\mu_k)$$\n",
        "The intensity distribution for each cluster k is Gaussian, described by $\\mu_k$ and $\\sigma_k$.\n",
        "\n",
        "The normalized intensity histogram can be depicted as a combination of Gaussians, defined by parameters $$\\Phi=(\\mu_k, \\sigma_k, c_k)_{k=1,\\ldots,K}.$$"
      ],
      "metadata": {
        "id": "9_jLQRWbhE5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Imagine you have a bunch of different colors, and you want to group them into certain categories based on how similar they are. In our case, we're grouping them based on their intensity, which is like how light or dark a color is.\n",
        "\n",
        "**1. Using Gaussian Distributions:**\n",
        "When we talk about representing the intensity distribution for each cluster (or category) using a Gaussian distribution, think of it like this: Imagine a bell curve. The top or peak of the curve represents the most common intensity for that group, and as we move away from the peak, the intensities become less common. This bell curve is defined by two things:\n",
        "- Its center (called the mean, represented by $\\mu$).\n",
        "- How spread out it is (called the variance, represented by $\\sigma^2$).\n",
        "\n",
        "**2. Likelihood of Observing an Intensity:**\n",
        "Now, when we see a certain intensity, $x_i$, in an image, we can use our bell curve to check how likely it is that this intensity belongs to a group $k$. The formula for this is:\n",
        "$$P(x_i|y_i=k,\\mu_k, \\sigma_k ) = \\frac{1}{\\sqrt{2\\pi \\sigma}} e^{\\frac{-(x_i-\\mu_k)^2}{2\\sigma_k^2}}$$\n",
        "This formula might look a bit intimidating, but it's just a mathematical way to describe our bell curve.\n",
        "\n",
        "**3. Mixture of Gaussians:**\n",
        "But what if our image has multiple groups of colors? This is where the idea of a mixture of Gaussians comes in. Instead of just one bell curve, we'll have several, each representing a different group of intensities. Each bell curve gets a weight, called the mixing coefficient ($c_k$), based on how dominant that group is in the image. We combine all these bell curves to get the overall distribution of intensities in the image:\n",
        "$$P(x|\\Phi) = \\sum_{k} c_k G_{\\sigma_k} (x−\\mu_k)$$\n",
        "\n",
        "**4. Parameters for Each Gaussian:**\n",
        "Lastly, for every bell curve (or Gaussian) we use, it's described by its center ($\\mu_k$), spread ($\\sigma_k$), and weight ($c_k$). We collectively represent all these parameters for all the bell curves as\n",
        "$$\\Phi = (\\mu_k, \\sigma_k, c_k)_{k=1,\\ldots,K}$$.\n",
        "\n",
        "---\n",
        "\n",
        "So, in simple terms, we're using a combination of bell curves to represent the distribution of intensities in an image. Each bell curve stands for a group of similar intensities, and the entire set of bell curves helps us understand the overall color distribution in the image."
      ],
      "metadata": {
        "id": "r5RSdMarhKoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Application: Breast Cancer Diagnosis**\n",
        "\n",
        "\n",
        "Introducing the Wisconsin breast cancer dataset, a valuable tool in the field of medical research and diagnostics.\n",
        "\n",
        "Here's a glimpse of what's inside: Cells, both from benign and malignant tissues, were carefully extracted using a biopsy technique known as a **\"fine needle aspirate.\"** Once extracted, they were then captured and converted into **digital photographs** with the help of a **microscope**.\n",
        "\n",
        "But it doesn’t stop there! We used an intresting method named **\"snakes\"** to interactively **detect the boundaries of these cells.** With these boundaries set, we were able to extract a variety of cell features. We've got information on **30 different features**, such as:\n",
        "\n",
        "- **Radius**: This refers to the **average distance from the cell's center to its perimeter**.\n",
        "\n",
        "- **Texture**: It's essentially the **standard deviation of the cell's intensities**.\n",
        "\n",
        "- **Area**: It’s **the number of pixels enclosed **within the cell boundary.\n",
        "\n",
        "For each biopsy specimen, we've accumulated statistics related to these features. This includes the **average value**, **standard error**, and even the **highest value observed**.\n",
        "\n",
        "Here’s the crucial bit: Every specimen comes with a diagnosis, indicating whether it's benign (non-cancerous) or malignant (cancerous).\n",
        "\n",
        "Now, while our dataset focuses on the traditional method of **hand-crafted feature extraction from images**, it's worth noting that modern algorithms are evolving. Many now autonomously learn relevant features from images through tools like **convolutional neural networks**.\n",
        "\n",
        "But for this exploration, our primary focus will be on **uncovering the hidden structures within the feature space**. Let's dive in!"
      ],
      "metadata": {
        "id": "5w_sFf07jyyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised learning\n",
        "\n",
        "Unsupervised learning is a type of machine learning where the algorithm is provided with input data that doesn't have labeled responses. The system tries to learn the patterns and the structure from the data without any supervised feedback.\n",
        "\n",
        "In simpler terms, imagine you have a jigsaw puzzle without the finished picture on the box. You have to figure out how to piece it together based solely on the shapes and images on each piece. That's a bit like unsupervised learning.\n",
        "\n",
        "Some common types of unsupervised learning methods include:\n",
        "\n",
        "1. **Clustering:** It's like grouping. The aim is to segregate groups with similar traits and assign them into clusters. Examples include K-means clustering where we try to group data into 'K' number of clusters.\n",
        "\n",
        "2. **Association:** Here, the algorithm identifies rules that describe large portions of the data, like people that buy X also tend to buy Y (think of the \"customers who bought this item also bought\" feature on shopping sites).\n",
        "\n",
        "Unsupervised learning contrasts with supervised learning, where the algorithm is trained on a dataset where the correct outputs are known and the algorithm makes predictions based on this prior knowledge."
      ],
      "metadata": {
        "id": "w4yizCk3nMX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering\n",
        "\n",
        "Imagine you have a big box of mixed-up marbles, and you want to group them based on their similarities. That's kind of what clustering does in the world of machine learning.\n",
        "\n",
        "Now, clustering is a part of \"unsupervised learning\". It's like trying to sort these marbles **without someone telling you what criteria to use**. Maybe you'd choose color, size, or material. You're unsupervised and figuring it out on your own.\n",
        "\n",
        "In the specific method we're talking about, called **\"K-means clustering\"**, the goal is pretty straightforward. Imagine each group of marbles as having a **\"middle point\" or \"mean\"**. We want to group them such that the marbles are **as close as possible** to their respective **\"middle point\"**. The closer the marbles in a group are to this central point, the better our group is!\n",
        "\n",
        "There's a fancy way to measure how good our groups are using a thing called 'within-class variance'. Without diving too deep, it's a way to look at **how spread out marbles are in each group**. For our marble sorting, we want groups where marbles are really close to each other and the \"middle point\" of their group. So, **lower variance means our groups are tighter and better!**\n",
        "\n",
        "Here's a formula (don't worry too much about it) that captures this idea:\n",
        "\n",
        "$$\\sigma^2 = \\sum_{k=1}^{K} \\frac{n_k}{N} \\sigma_k^2$$\n",
        "\n",
        "This formula represents the **weighted average of the variances of each group** (or cluster) in the K-means clustering method.\n",
        "\n",
        "- $n$: number of examples in class k\n",
        "\n",
        "- $\\sigma$: variance if class k\n",
        "\n",
        "- $N$: total number of samples\n",
        "\n",
        "In simpler words, it's just a way to measure the average spread of marbles in all our groups. The goal is to get this value as small as possible!\n"
      ],
      "metadata": {
        "id": "O3vumVqClz7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, let's dive into K-means clustering in a more approachable way:\n",
        "\n",
        "**Understanding K-means Clustering**\n",
        "\n",
        "When we talk about K-means clustering, we're essentially trying to group data points into clusters where the data points in each cluster are similar to each other. One way to measure this similarity is by minimizing the differences (or variance) within each cluster.\n",
        "\n",
        "Here's a simple way to think about it: Imagine you're trying to organize books on a shelf. You want books of the same genre to be together. The \"distance\" between each book could be the difference in their topics. In K-means, we're doing something similar, but with data points instead of books.\n",
        "\n",
        "**The Process**\n",
        "\n",
        "1. **Initialization**: We start by randomly picking some points as the initial centers of our clusters.\n",
        "2. **Assigning Clusters**: For each data point, we check which of our cluster centers it's closest to in terms of its features (like the genre in our book example) and assign it to that cluster.\n",
        "3. **Recalculating Cluster Centers**: After assigning all points to clusters, we calculate the average of all points in a cluster and set this as the new center.\n",
        "4. **Repeat**: We go back to step 2 and continue until our cluster centers don't change much.\n",
        "\n",
        "A cool thing to note: The variance within each cluster is like the average \"distance\" of every data point in that cluster to its center. Our aim is to make this \"distance\" as small as possible.\n",
        "\n",
        "**The Math Behind It**\n",
        "\n",
        "The formula you provided calculates this \"distance\" or variance:\n",
        "\n",
        "$$ 𝜎^2 = \\frac{1}{𝑁} ∑_(𝑘=1)^𝐾 ∑_(𝑖∈𝑆_𝑘) |𝒙_𝑖−𝝁_𝑘 |^2 $$\n",
        "\n",
        "Here:\n",
        "- \\(𝒙_𝑖\\) represents the feature of a specific data point.\n",
        "- \\(𝝁_(𝑧_𝑖)\\) is the average feature of the cluster to which the data point belongs.\n",
        "- The symbol |...| represents the Euclidean distance, which is just a way of measuring the straight-line distance between two points.\n",
        "\n",
        "In essence, this formula adds up all the distances of the data points to their respective cluster centers and then averages them.\n",
        "\n"
      ],
      "metadata": {
        "id": "Md6qTP0mnaJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More about the formula:\n",
        "\n",
        "$$ 𝜎^2 = \\frac{1}{𝑁} ∑_(𝑘=1)^𝐾 ∑_(𝑖∈𝑆_𝑘) |𝒙_𝑖−𝝁_𝑘 |^2 $$\n",
        "\n",
        "1. **\\(𝜎^2\\)**: This represents the total within-class variance. Think of it as the average \"distance\" of each data point to the center of its assigned cluster.\n",
        "\n",
        "2. **∑_(𝑘=1)^𝐾**: This is a summation symbol that runs from 1 to \\(𝐾\\), where \\(𝐾\\) is the number of clusters. It means we're adding up values for each cluster.\n",
        "\n",
        "3. **\\(𝑛_𝑘\\)**: This represents the number of data points in cluster \\(𝑘\\).\n",
        "\n",
        "4. **𝑁**: This is the total number of data points across all clusters.\n",
        "\n",
        "5. **\\(𝑛_𝑘/𝑁\\)**: This calculates the proportion of data points in cluster \\(𝑘\\) relative to the total number of data points.\n",
        "\n",
        "6. **1/𝑛_𝑘**: This is the inverse of the number of data points in cluster \\(𝑘\\). It essentially gives weight to the variance based on cluster size.\n",
        "\n",
        "7. **∑_(𝑖∈𝑆_𝑘)**: Another summation, but this one sums over all the data points \\(𝑖\\) that belong to cluster \\(𝑘\\).\n",
        "\n",
        "8. **|𝒙_𝑖−𝝁_𝑘 |^2**: This represents the squared Euclidean distance between a data point \\(𝒙_𝑖\\) and the mean of its assigned cluster \\(𝝁_𝑘\\). It measures how far a data point is from the center of its cluster.\n",
        "\n",
        "9. **𝝁_(𝑧_𝑖 )**: This is the mean feature vector for the class assigned to data point \\(𝒙_𝑖\\).\n",
        "\n",
        "Breaking it all down, the formula calculates the average of the squared distances of each data point to the center of its assigned cluster, and then sums these averages across all clusters to get the total within-class variance. It's a measure of how spread out the data points are within their assigned clusters."
      ],
      "metadata": {
        "id": "0wteaUBBrY-z"
      }
    }
  ]
}