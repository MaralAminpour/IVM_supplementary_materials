{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeLlpvgwLgXa/saF9zI974",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/IVM_supplementary_materials/blob/main/Clustering_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will represent the intensity distribution for each cluster using a Gaussian distribution. This can be viewed as the likelihood of observing intensity $x_i$ from a sample in class k. This Gaussian distribution is characterized by two parameters: the mean $\\mu$ and the variance $\\sigma^2$.\n",
        "\n",
        "The observed intensity distribution of the entire image, represented by the normalized histogram, can be perceived as the likelihood of observing intensity x.\n",
        "\n",
        "We represent this likelihood distribution as a blend of Gaussian distributions, each multiplied by their respective mixing coefficients, and then summed across all classes.\n",
        "$$P(x_i|y_i=k,\\mu_k, \\sigma_k )=G_{\\sigma_k} (x_i−\\mu_k)=\\frac{1}{\\sqrt{2\\pi \\sigma}} e^{\\frac{-(x_i-\\mu_k)^2}{2\\sigma_k^2}}$$\n",
        "$$P(x|\\Phi)=\\sum_{k} c_k G_{\\sigma_k} (x−\\mu_k)$$\n",
        "The intensity distribution for each cluster k is Gaussian, described by $\\mu_k$ and $\\sigma_k$.\n",
        "\n",
        "The normalized intensity histogram can be depicted as a combination of Gaussians, defined by parameters $$\\Phi=(\\mu_k, \\sigma_k, c_k)_{k=1,\\ldots,K}.$$"
      ],
      "metadata": {
        "id": "9_jLQRWbhE5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Imagine you have a bunch of different colors, and you want to group them into certain categories based on how similar they are. In our case, we're grouping them based on their intensity, which is like how light or dark a color is.\n",
        "\n",
        "**1. Using Gaussian Distributions:**\n",
        "When we talk about representing the intensity distribution for each cluster (or category) using a Gaussian distribution, think of it like this: Imagine a bell curve. The top or peak of the curve represents the most common intensity for that group, and as we move away from the peak, the intensities become less common. This bell curve is defined by two things:\n",
        "- Its center (called the mean, represented by $\\mu$).\n",
        "- How spread out it is (called the variance, represented by $\\sigma^2$).\n",
        "\n",
        "**2. Likelihood of Observing an Intensity:**\n",
        "Now, when we see a certain intensity, $x_i$, in an image, we can use our bell curve to check how likely it is that this intensity belongs to a group $k$. The formula for this is:\n",
        "$$P(x_i|y_i=k,\\mu_k, \\sigma_k ) = \\frac{1}{\\sqrt{2\\pi \\sigma}} e^{\\frac{-(x_i-\\mu_k)^2}{2\\sigma_k^2}}$$\n",
        "This formula might look a bit intimidating, but it's just a mathematical way to describe our bell curve.\n",
        "\n",
        "**3. Mixture of Gaussians:**\n",
        "But what if our image has multiple groups of colors? This is where the idea of a mixture of Gaussians comes in. Instead of just one bell curve, we'll have several, each representing a different group of intensities. Each bell curve gets a weight, called the mixing coefficient ($c_k$), based on how dominant that group is in the image. We combine all these bell curves to get the overall distribution of intensities in the image:\n",
        "$$P(x|\\Phi) = \\sum_{k} c_k G_{\\sigma_k} (x−\\mu_k)$$\n",
        "\n",
        "**4. Parameters for Each Gaussian:**\n",
        "Lastly, for every bell curve (or Gaussian) we use, it's described by its center ($\\mu_k$), spread ($\\sigma_k$), and weight ($c_k$). We collectively represent all these parameters for all the bell curves as\n",
        "$$\\Phi = (\\mu_k, \\sigma_k, c_k)_{k=1,\\ldots,K}$$.\n",
        "\n",
        "---\n",
        "\n",
        "So, in simple terms, we're using a combination of bell curves to represent the distribution of intensities in an image. Each bell curve stands for a group of similar intensities, and the entire set of bell curves helps us understand the overall color distribution in the image."
      ],
      "metadata": {
        "id": "r5RSdMarhKoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Application: Breast Cancer Diagnosis**\n",
        "\n",
        "\n",
        "Introducing the Wisconsin breast cancer dataset, a valuable tool in the field of medical research and diagnostics.\n",
        "\n",
        "Here's a glimpse of what's inside: Cells, both from benign and malignant tissues, were carefully extracted using a biopsy technique known as a **\"fine needle aspirate.\"** Once extracted, they were then captured and converted into **digital photographs** with the help of a **microscope**.\n",
        "\n",
        "But it doesn’t stop there! We used an intresting method named **\"snakes\"** to interactively **detect the boundaries of these cells.** With these boundaries set, we were able to extract a variety of cell features. We've got information on **30 different features**, such as:\n",
        "\n",
        "- **Radius**: This refers to the **average distance from the cell's center to its perimeter**.\n",
        "\n",
        "- **Texture**: It's essentially the **standard deviation of the cell's intensities**.\n",
        "\n",
        "- **Area**: It’s **the number of pixels enclosed **within the cell boundary.\n",
        "\n",
        "For each biopsy specimen, we've accumulated statistics related to these features. This includes the **average value**, **standard error**, and even the **highest value observed**.\n",
        "\n",
        "Here’s the crucial bit: Every specimen comes with a diagnosis, indicating whether it's benign (non-cancerous) or malignant (cancerous).\n",
        "\n",
        "Now, while our dataset focuses on the traditional method of **hand-crafted feature extraction from images**, it's worth noting that modern algorithms are evolving. Many now autonomously learn relevant features from images through tools like **convolutional neural networks**.\n",
        "\n",
        "But for this exploration, our primary focus will be on **uncovering the hidden structures within the feature space**. Let's dive in!"
      ],
      "metadata": {
        "id": "5w_sFf07jyyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised learning\n",
        "\n",
        "Unsupervised learning is a type of machine learning where the algorithm is provided with input data that doesn't have labeled responses. The system tries to learn the patterns and the structure from the data without any supervised feedback.\n",
        "\n",
        "In simpler terms, imagine you have a jigsaw puzzle without the finished picture on the box. You have to figure out how to piece it together based solely on the shapes and images on each piece. That's a bit like unsupervised learning.\n",
        "\n",
        "Some common types of unsupervised learning methods include:\n",
        "\n",
        "1. **Clustering:** It's like grouping. The aim is to segregate groups with similar traits and assign them into clusters. Examples include K-means clustering where we try to group data into 'K' number of clusters.\n",
        "\n",
        "2. **Association:** Here, the algorithm identifies rules that describe large portions of the data, like people that buy X also tend to buy Y (think of the \"customers who bought this item also bought\" feature on shopping sites).\n",
        "\n",
        "Unsupervised learning contrasts with supervised learning, where the algorithm is trained on a dataset where the correct outputs are known and the algorithm makes predictions based on this prior knowledge."
      ],
      "metadata": {
        "id": "w4yizCk3nMX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering\n",
        "\n",
        "Imagine you have a big box of mixed-up marbles, and you want to group them based on their similarities. That's kind of what clustering does in the world of machine learning.\n",
        "\n",
        "Now, clustering is a part of \"unsupervised learning\". It's like trying to sort these marbles without someone telling you what criteria to use. Maybe you'd choose color, size, or material. You're unsupervised and figuring it out on your own.\n",
        "\n",
        "In the specific method we're talking about, called \"K-means clustering\", the goal is pretty straightforward. Imagine each group of marbles as having a \"middle point\" or \"mean\". We want to group them such that the marbles are as close as possible to their respective \"middle point\". The closer the marbles in a group are to this central point, the better our group is!\n",
        "\n",
        "There's a fancy way to measure how good our groups are using a thing called 'within-class variance'. Without diving too deep, it's a way to look at how spread out marbles are in each group. For our marble sorting, we want groups where marbles are really close to each other and the \"middle point\" of their group. So, lower variance means our groups are tighter and better!\n",
        "\n",
        "Here's a formula (don't worry too much about it) that captures this idea:\n",
        "\n",
        "Of course, here's the formula you've requested:\n",
        "\n",
        "$$\\sigma^2 = \\sum_{k=1}^{K} \\frac{n_k}{N} \\sigma_k^2$$\n",
        "\n",
        "This formula represents the weighted average of the variances of each group (or cluster) in the K-means clustering method.\n",
        "\n",
        "In simpler words, it's just a way to measure the average spread of marbles in all our groups. The goal is to get this value as small as possible!\n",
        "\n",
        "Hope that helps clarify things a bit! Remember, it's all about grouping things (or data) based on their similarities, much like sorting marbles."
      ],
      "metadata": {
        "id": "O3vumVqClz7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Md6qTP0mnaJi"
      }
    }
  ]
}