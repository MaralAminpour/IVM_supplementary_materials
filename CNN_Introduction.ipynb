{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeTMLxCNuYsjRnKwxJuRjG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/IVM_supplementary_materials/blob/main/CNN_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computational motivations"
      ],
      "metadata": {
        "id": "fukm9Z_5b7We"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regular neural networks cannot scale to full images!\n",
        "\n",
        "\n",
        "\"MLP\" is an acronym for \"Multi-Layer Perceptron,\" which is a type of neural network characterized by multiple layers through which data passes in a feedforward manner. In the recent lecture, we explored how an MLP, also known as a fully connected network, can be trained to classify images from the MNIST dataset. The MNIST dataset comprises small, 2-dimensional grayscale images of handwritten digits, each with a resolution of 28 by 28 pixels.\n",
        "\n",
        "In contrast, the image dimensions we encounter in modern applications can be much larger. For instance, a standard JPEG image might have dimensions of 640 by 480 pixels, with an additional dimension for color channels, typically resulting in a 3-dimensional array of 640 x 480 x 3. Similarly, medical imaging, such as MRI scans of the brain, might produce volumetric data with dimensions reaching 300 x 300 x 200 voxels.\n",
        "\n",
        "If one were to connect every voxel in such large images in a fully connected network, the number of parameters to learn would be astronomically high. For example, a single fully connected layer that connects every voxel from an MRI scan with dimensions of 300 x 300 x 200 would entail learning a weight matrix with around 18 million parameters. This situation, where the number of features dwarfs the number of training examples, can lead to overfitting, where the model learns the training data too well, including its noise and anomalies. Consequently, such a model would likely perform poorly when generalizing to new, unseen data. This is a significant concern in machine learning, as it undermines the purpose of creating models that can predict and perform well on real-world data.\n",
        "\n",
        "Moreover, the utilization of fully connected layers for image data is excessively redundant. Unlike linear regression, which treats each input feature independently, image data possesses inherent structure where adjacent pixels or voxels are often correlated. This spatial correlation is a crucial piece of information that fully connected networks typically ignore.\n",
        "\n",
        "However, it is important to recognize that images are more than just collections of correlated pixels; they can be conceptualized as hierarchies of increasingly complex patterns or textures. For instance, the Fourier transform encoding of magnetic resonance images exemplifies this by decomposing images into patterns of varying spatial frequencies, from low-level textures to more complex structures. This concept is not limited to medical imaging; it also applies to natural images. The hierarchical nature of image data suggests that a different network architecture, such as convolutional neural networks (CNNs), might be more appropriate. CNNs leverage the correlated spatial information and the hierarchical structure of images by using convolutional filters to capture patterns within localized regions, allowing for a significant reduction in the number of parameters and better generalization capabilities."
      ],
      "metadata": {
        "id": "fip8n2aUbJud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taking inspiration from human vision\n",
        "\n",
        "The insightful observation that the mammalian brain efficiently processes visual information has inspired the structure and function of modern image recognition systems. When light reaches our eyes, the captured signals are transmitted from the retina to the primary visual cortex, known as V1, located at the back of the brain. V1 serves as a topographical map for visual stimuli: points that are close together in the visual field are processed by adjacent neurons in V1. The neurons here are adept at detecting edges and other high-frequency spatial features, acting as specialized edge detectors.\n",
        "\n",
        "As visual information progresses through the brain's visual pathway, it is relayed to subsequent cortical areas, such as V2 and V4. Each of these regions extracts and processes more complex patterns, building on the simple features identified by V1. This hierarchical processing is essential, as it allows the brain to handle complex visual tasks, such as object recognition and motion detection. For instance, object recognition involves higher-order visual areas that evolve to represent increasingly complex features until the signals reach the inferior temporal cortex. In this region, individual neurons can be highly selective, responding vigorously to specific objects, like faces.\n",
        "\n",
        "Convolutional Neural Networks (CNNs) are crafted to emulate this hierarchical processing structure. In a CNN, multiple layers of convolutional filters are applied to input images, where initial layers may resemble the function of V1 by detecting simple edges and textures. As the data passes through successive layers, the network identifies more intricate patterns, eventually recognizing whole objects within the images.\n",
        "\n",
        "This layered approach offers a substantial advantage: CNNs do not rely on spatial normalization or image registration. Therefore, there is no need to align images to a standard form or assume that corresponding pixels across different images represent identical content. This flexibility is crucial, especially when dealing with varied image presentations where direct pixel-to-pixel comparison is not feasible due to transformations such as scaling, rotation, or skewing.\n",
        "\n",
        "This characteristic of CNNs is particularly beneficial when compared to traditional machine learning (ML) techniques, which often treat input examples as flat feature vectors, comparing each feature based solely on its position within the vector. This method can suffice for simple, consistent datasets like MNIST, where the images are relatively uniform in size and position. However, it becomes impractical for more complex or varied datasets, such as those involving natural scenes or biological structures, where the spatial relationships and features cannot be neatly mapped onto a fixed vector space without losing critical structural information.\n",
        "\n",
        "Thus, CNNs represent a significant advancement over previous hand-engineered feature detection methods. They allow for the learning of features in a way that respects the intrinsic variability and complexity of the visual world, much like our own biological visual processing systems."
      ],
      "metadata": {
        "id": "1p7E4H7fcMvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**More simple explanation:** Think of it like this: When you look at a picture, your brain doesn't just see a bunch of pixels; **it sees shapes, edges, and patterns**. This happens because the visual information from your eyes goes on a bit of a journey inside your brain, starting at a place called V1. This is where your brain starts to **make sense of all** the lines and edges in what you're looking at.\n",
        "\n",
        "From there, the info hops from one brain spot to another, with each stop getting better at figuring out what you're seeing — from simple patterns all the way up to complex stuff like recognizing your best friend's face in a crowd.\n",
        "\n",
        "**Now, imagine trying to teach a computer to do that.** That's where Convolutional Neural Networks (CNNs) come in. They're like a computer version of your brain's visual journey. In a CNN, the first layers are like your brain's V1 area — **good at spotting edges and basic patterns.** As you go **deeper** into the network, it **gets better** at seeing more complicated things, like shapes and eventually whole objects, **without getting confused if the picture is tilted or a little blurry.**\n",
        "\n",
        "This is super cool because, unlike older computer vision methods that needed everything lined up perfectly, CNNs can deal with pictures being all sorts of different sizes and angles — just like we do when we see something new.\n",
        "\n",
        "So, for simple stuff like the MNIST dataset where you have **digits neatly centered and looking pretty similar, old-school methods were fine.** But throw in a photo from your last vacation or a medical image, and things get trickier. **This is where CNNs really shine, handling all the messy, real-world variability** like champs, much like our own nifty brains do."
      ],
      "metadata": {
        "id": "2aSTC0X2dDZ3"
      }
    }
  ]
}