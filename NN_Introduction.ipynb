{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYZ1+l19iqVX/4CeyzcbVl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/IVM_supplementary_materials/blob/main/NN_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neuron structure\n",
        "\n",
        "- **Dendrites**: Think of these as the neuron's 'inbox' – they receive messages in the form of chemical signals from other neurons.\n",
        "- **Soma**: This is the 'main office' of the neuron where all the incoming messages get read and sorted out. It's like the boss deciding if there's enough reason to pass on a message.\n",
        "- **Axon**: Consider this as the 'delivery guy' – if the soma gives a thumbs up, the axon carries the 'go' signal down the line to other neurons.\n",
        "- **Myelin Sheath**: Some axons have this special 'insulation tape' wrapped around them, helping the 'go' signal travel faster and further without getting weaker.\n",
        "- **Synapses**: These are the 'meet-and-greet' spots where one neuron connects to another. They're the social hubs of the neuron world.\n",
        "- **Chemical Synapses**: Picture a tiny space where the pre-synaptic neuron (the sender) doesn't actually touch the post-synaptic neuron (the receiver). Instead, it sends chemical messengers across this gap to pass on its message.\n",
        "- **Neurotransmitters**: These are the 'text messages' sent by the pre-synaptic neuron, which tell the receiving neuron what to do next – either get excited and send its own message or chill out and remain quiet.\n",
        "- **Excitatory vs. Inhibitory Synapses**: It's like having two kinds of text messages – one kind gets you all hyped up to do something ('Let's go!'), and the other is more like a calming message to take it easy ('Relax, no rush').\n",
        "\n",
        "In summary, neurons are like tiny, busy offices that take in information, decide what's important, and then pass on messages to the next neuron in line. The whole process is a mix of receiving signals, making decisions, and sending responses."
      ],
      "metadata": {
        "id": "u-oEhzzL_Cg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **Dendrites**: These are extensions of the neuron that act as the input channels. They receive chemical signals from other neurons and convert them into electrical signals.\n",
        "- **Soma**: Also known as the cell body, this part of the neuron integrates the electrical signals received from dendrites to determine if the neuron will activate and send a signal along to other neurons.\n",
        "- **Axon**: This is a long, slender projection that transmits the electrical signal (action potential) away from the neuron's soma toward other neurons.\n",
        "- **Myelin Sheath**: Some axons are wrapped in this protective sheath, which helps speed up the transmission of the action potential over long distances.\n",
        "- **Synapses**: These are the junctions where neurons communicate with each other, transferring information from one neuron to the next.\n",
        "- **Chemical Synapses**: In this type, there's a small gap between neurons where the signal is transferred using chemical messengers called neurotransmitters, which are released from one neuron and bind to receptors on the next.\n",
        "- **Neurotransmitters**: These chemicals can either excite the next neuron, prompting it to send a signal, or inhibit it from sending a signal.\n",
        "- **Excitatory and Inhibitory Synapses**: These are the two types of chemical synapses. Excitatory synapses encourage the next neuron to send a signal, while inhibitory synapses discourage it from doing so.\n",
        "\n",
        "Each component of the neuron plays a critical role in processing and transmitting information throughout the nervous system."
      ],
      "metadata": {
        "id": "PUi6t-aECGfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Biological Neural Networks (BNNs) and Artificial Neural Networks (ANNs) have distinct parts that correspond to each other, underpinning their conceptual similarities:\n",
        "\n",
        "**Parts of Biological Neural Networks:**\n",
        "\n",
        "- **Neurons**: The fundamental cells that process and transmit information through electrical and chemical signals.\n",
        "- **Dendrites**: Receive signals from other neurons.\n",
        "- **Soma (Cell Body)**: Integrates incoming signals to determine if the neuron will activate.\n",
        "- **Axon**: Transmits the electrical signal to other neurons.\n",
        "- **Synapses**: Junctions where neurons communicate, using neurotransmitters to send signals.\n",
        "- **Myelin Sheath**: Insulation around some axons that speeds up signal transmission.\n",
        "- **Neurotransmitters**: Chemicals that transmit signals across synapses.\n",
        "\n",
        "**Parts of Artificial Neural Networks:**\n",
        "\n",
        "- **Artificial Neurons (Nodes)**: Basic processing units that simulate biological neurons.\n",
        "- **Inputs**: Analogous to dendrites, they receive data to be processed.\n",
        "- **Weights**: Equivalent to the strength of synaptic connections, determining the influence of inputs.\n",
        "- **Activation Function**: Serves a similar purpose as the soma, deciding the level of output signal based on input strength.\n",
        "- **Outputs**: Correspond to the axon, transmitting the signal to the next layer or as a final output.\n",
        "- **Layers**: Structured groupings of nodes; including input, hidden, and output layers.\n",
        "- **Learning Algorithm (e.g., Backpropagation)**: Method for adjusting weights in the network, similar to how experiences rewire synaptic connections.\n",
        "\n",
        "**Similarities:**\n",
        "\n",
        "- **Signal Processing**: Both BNNs and ANNs process information through a network of interconnected units (neurons/nodes).\n",
        "- **Adaptation**: Neurons in BNNs adapt through changes in synaptic strength, while ANNs adapt through changes in weights.\n",
        "- **Integration and Activation**: Neurons integrate signals and fire based on a threshold; similarly, nodes calculate weighted sums and apply an activation function.\n",
        "- **Transmission**: Just as axons transmit signals to other neurons, ANNs transmit processed data from one node to the next.\n",
        "- **Learning**: Both networks learn from repeated exposure to stimuli (data), although the mechanisms differ (biological processes vs. computational algorithms).\n",
        "\n",
        "The conceptual similarity is rooted in the inspiration ANNs take from BNNs, using an abstracted and simplified model to replicate the complex patterns of data processing and learning observed in biological systems."
      ],
      "metadata": {
        "id": "Iqgleg_I_4iy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Artificial Neuron Basics**: They're simplified versions of biological neurons, where the complex brain processes are approximated in a limited fashion.\n",
        "- **Signal Multiplication**: Inputs (\\(x_i\\)) are multiplied by corresponding weights (\\(w_i\\)) to represent the synaptic strength in the network.\n",
        "- **Adding Bias**: A bias term is added to shift the activation threshold away from zero, adjusting the response of the neuron.\n",
        "- **Activation Function**: An artificial neuron 'fires' if the weighted sum of inputs, after including the bias, passes through a nonlinear activation function (\\(f\\)) and exceeds a certain threshold.\n",
        "- **Learning Process**: Neural networks learn by adjusting weights and biases to improve performance on specific tasks.\n",
        "- **Engineering vs. Biology**: While artificial neurons are inspired by biology, their design in fields like computer vision is often driven by engineering needs rather than an attempt to closely simulate brain function.\n",
        "- **Distinct Research**: There are networks explicitly designed to mimic biological processes, but these biologically inspired networks are a different branch of study within the broader field of artificial intelligence."
      ],
      "metadata": {
        "id": "TZV9mmiqCQ7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Artificial neurons in a neural network are meant to somewhat replicate the way biological neurons work, but the key word here is \"somewhat.\" They do this by taking inputs (think of these as signals or pieces of information) and assigning each one a weight—this is similar to how biological neurons have different strengths of connections. These weights are like the volume knobs for each input, determining how much each signal should be amplified or dialed down.\n",
        "\n",
        "Then there's the bias. You can think of the bias as setting the baseline or starting point for where the decision-making begins. It's not just about whether the signals add up to a positive or negative number; the bias adjusts the level at which the neuron's output becomes significant enough to be considered.\n",
        "\n",
        "After all the inputs have been adjusted by their weights and the bias is added, we don't just add them up to get a result. Instead, we pass this sum through a special function called the activation function. This function is designed to introduce complexity into the equation, allowing the neuron to make more nuanced decisions than just a simple yes or no. It's like deciding whether to forward an email based on a quick scan—it needs to be interesting enough, not just the first one in your inbox.\n",
        "\n",
        "The whole goal of a neural network is to figure out the best weights and biases to use for making predictions or decisions, which is done during the learning phase. The network adjusts these weights and biases little by little, each time it looks at new data, trying to get better at whatever job it's supposed to do.\n",
        "\n",
        "While all this is inspired by how our brains work, artificial neurons are a lot simpler and more focused on specific tasks than the vast complexity of human neurons. The design choices in artificial neural networks, especially in areas like computer vision, are more about engineering the best solution for a problem rather than mimicking the human brain's functionality. There are some neural networks that really try to get close to biological reality, but they're a different kind of project and aren't as common in everyday tech."
      ],
      "metadata": {
        "id": "M57HG7qkEVjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Artificial neurons are the building blocks of machine learning models, and they draw inspiration from the human brain's neurons. But let's be clear, they're a simplified version. Each artificial neuron acts like a tiny filter for whatever data you throw at it, deciding what's important and what's not by assigning weights to the inputs—kind of like giving different levels of attention to the information it receives.\n",
        "\n",
        "Now, let's break down the process:\n",
        "\n",
        "- We start with inputs, which are your raw data points or features. Each one is paired with a weight, representing its importance in the decision-making process. It's a bit like focusing on specific aspects of a situation to make a decision.\n",
        "- Next up is the bias. This is like a built-in judgment that shifts the starting point of the calculation. It ensures that the neuron doesn't activate (or fire) for just any input combination. There's a threshold that needs to be crossed, which the bias helps to set.\n",
        "- After the inputs are weighted and the bias is added, we don't just sum it up. This total goes through an activation function, which decides whether the neuron should activate. This function allows the neuron to pick up on more complex patterns by introducing nonlinearity to the system.\n",
        "- The real magic happens during the learning phase. The network tweaks the weights and biases in response to the errors it makes, slowly improving its accuracy and decision-making capabilities. This is somewhat similar to learning from mistakes and experiences, although it's all about calculations and adjustments in the artificial setup.\n",
        "\n",
        "Artificial neurons are focused on efficiency and solving specific tasks, often taking a more engineered approach rather than trying to mimic the exact way our brains work. While some neural networks aim to be more like our brain's neural networks, they're usually part of specialized research and not what you typically see in everyday tech applications."
      ],
      "metadata": {
        "id": "ol1Pk5mCH-TI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The human brain is an incredibly complex network comprised of around 86 billion neurons, all linked together by an estimated 100 trillion to 1 quadrillion synapses. The accompanying illustration simplifies this complexity by showing a sketch of a biological neuron next to its mathematical representation.\n",
        "\n",
        "In essence, a neuron processes incoming signals at its dendrites and sends outgoing signals down its axon. This axon then splits and forms connections, through synapses, with the dendrites of other neurons. In the mathematical model we use for computation, the signals (like $ x_0 $) moving along the axon are modified by weights $( w_0 $) representing the synapse's strength, which determines how much one neuron will influence another. These weights are adjustable, allowing the system to learn and determine the impact of one neuron's activity on another, whether it be stimulating (positive weight) or dampening (negative weight).\n",
        "\n",
        "In this model, signals received by the dendrites are summed up in the cell body. If this total surpasses a specific limit, the neuron 'fires', sending a signal down its axon. Simplifying further for computational purposes, we don't consider the exact timing of these signals; instead, we focus on the rate of firing. This rate is modeled using an activation function, denoted as $ f $, to represent the frequency of the output signals.\n",
        "\n",
        "Historically, the sigmoid function $( \\sigma $) has been a popular choice for an activation function because it effectively compresses a real-valued input—the accumulated signal strength—into a range between 0 and 1. Later on, we'll dive into the specifics of various activation functions and their roles in neural computation."
      ],
      "metadata": {
        "id": "q4Gu3zOfMjlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Biological motivation and connections\n",
        "The basic computational unit of the brain is a neuron. Approximately 86 billion neurons can be found in the human nervous system and they are connected with approximately 10^14 - 10^15 synapses. The diagram below shows a cartoon drawing of a biological neuron (left) and a common mathematical model (right). Each neuron receives input signals from its dendrites and produces output signals along its (single) axon. The axon eventually branches out and connects via synapses to dendrites of other neurons. In the computational model of a neuron, the signals that travel along the axons (e.g. x0) interact multiplicatively (e.g. w0x0) with the dendrites of the other neuron based on the synaptic strength at that synapse (e.g. w0). The idea is that the synaptic strengths (the weights w) are learnable and control the strength of influence (and its direction: excitory (positive weight) or inhibitory (negative weight)) of one neuron on another. In the basic model, the dendrites carry the signal to the cell body where they all get summed. If the final sum is above a certain threshold, the neuron can fire, sending a spike along its axon. In the computational model, we assume that the precise timings of the spikes do not matter, and that only the frequency of the firing communicates information. Based on this rate code interpretation, we model the firing rate of the neuron with an activation function f, which represents the frequency of the spikes along the axon. Historically, a common choice of activation function is the sigmoid function σ, since it takes a real-valued input (the signal strength after the sum) and squashes it to range between 0 and 1. We will see details of these activation functions later in this section."
      ],
      "metadata": {
        "id": "AwRgiSuCPjvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.itechcreations.in/artificial-intelligence/artificial-neural-network-for-dummies-an-introduction/\n",
        "\n",
        "https://www.upgrad.com/blog/neural-networks-for-dummies-a-comprehensive-guide/\n",
        "\n",
        "https://ai.plainenglish.io/neural-networks-for-dummies-841a404be413\n",
        "\n",
        "https://talendor.io/neural-networks-for-dummies\n",
        "\n",
        "https://www.freecodecamp.org/news/neural-networks-for-dummies-a-quick-intro-to-this-fascinating-field-795b1705104a/\n",
        "\n"
      ],
      "metadata": {
        "id": "6NeQbQABnCec"
      }
    }
  ]
}